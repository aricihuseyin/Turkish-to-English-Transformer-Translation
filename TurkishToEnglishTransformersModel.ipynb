{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Cf8qp1u0Kw",
        "outputId": "b8d03e40-b26a-4a2d-a837-3d798e79dcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [1/2603], Loss: 10.672295570373535\n",
            "Epoch [1/5], Step [11/2603], Loss: 8.48673152923584\n",
            "Epoch [1/5], Step [21/2603], Loss: 7.570736408233643\n",
            "Epoch [1/5], Step [31/2603], Loss: 6.881667137145996\n",
            "Epoch [1/5], Step [41/2603], Loss: 6.284490585327148\n",
            "Epoch [1/5], Step [51/2603], Loss: 5.789129734039307\n",
            "Epoch [1/5], Step [61/2603], Loss: 5.5774407386779785\n",
            "Epoch [1/5], Step [71/2603], Loss: 5.571192264556885\n",
            "Epoch [1/5], Step [81/2603], Loss: 5.478513717651367\n",
            "Epoch [1/5], Step [91/2603], Loss: 5.3568854331970215\n",
            "Epoch [1/5], Step [101/2603], Loss: 5.2502055168151855\n",
            "Epoch [1/5], Step [111/2603], Loss: 5.2531890869140625\n",
            "Epoch [1/5], Step [121/2603], Loss: 5.186863899230957\n",
            "Epoch [1/5], Step [131/2603], Loss: 4.927368640899658\n",
            "Epoch [1/5], Step [141/2603], Loss: 5.029376983642578\n",
            "Epoch [1/5], Step [151/2603], Loss: 5.169938087463379\n",
            "Epoch [1/5], Step [161/2603], Loss: 5.059008598327637\n",
            "Epoch [1/5], Step [171/2603], Loss: 4.820137023925781\n",
            "Epoch [1/5], Step [181/2603], Loss: 4.989614009857178\n",
            "Epoch [1/5], Step [191/2603], Loss: 4.747291088104248\n",
            "Epoch [1/5], Step [201/2603], Loss: 4.515301704406738\n",
            "Epoch [1/5], Step [211/2603], Loss: 4.570568561553955\n",
            "Epoch [1/5], Step [221/2603], Loss: 4.754263401031494\n",
            "Epoch [1/5], Step [231/2603], Loss: 4.273551940917969\n",
            "Epoch [1/5], Step [241/2603], Loss: 4.305293083190918\n",
            "Epoch [1/5], Step [251/2603], Loss: 4.598282337188721\n",
            "Epoch [1/5], Step [261/2603], Loss: 4.356614589691162\n",
            "Epoch [1/5], Step [271/2603], Loss: 4.409882545471191\n",
            "Epoch [1/5], Step [281/2603], Loss: 4.346984386444092\n",
            "Epoch [1/5], Step [291/2603], Loss: 4.4349799156188965\n",
            "Epoch [1/5], Step [301/2603], Loss: 4.438729763031006\n",
            "Epoch [1/5], Step [311/2603], Loss: 4.246884822845459\n",
            "Epoch [1/5], Step [321/2603], Loss: 4.568726062774658\n",
            "Epoch [1/5], Step [331/2603], Loss: 4.050447940826416\n",
            "Epoch [1/5], Step [341/2603], Loss: 3.9707393646240234\n",
            "Epoch [1/5], Step [351/2603], Loss: 4.36435079574585\n",
            "Epoch [1/5], Step [361/2603], Loss: 4.380183696746826\n",
            "Epoch [1/5], Step [371/2603], Loss: 3.897575616836548\n",
            "Epoch [1/5], Step [381/2603], Loss: 4.350208282470703\n",
            "Epoch [1/5], Step [391/2603], Loss: 4.210937023162842\n",
            "Epoch [1/5], Step [401/2603], Loss: 4.0152082443237305\n",
            "Epoch [1/5], Step [411/2603], Loss: 4.150137424468994\n",
            "Epoch [1/5], Step [421/2603], Loss: 3.9498651027679443\n",
            "Epoch [1/5], Step [431/2603], Loss: 4.123100280761719\n",
            "Epoch [1/5], Step [441/2603], Loss: 4.1149492263793945\n",
            "Epoch [1/5], Step [451/2603], Loss: 3.963738441467285\n",
            "Epoch [1/5], Step [461/2603], Loss: 3.9631776809692383\n",
            "Epoch [1/5], Step [471/2603], Loss: 3.9493725299835205\n",
            "Epoch [1/5], Step [481/2603], Loss: 4.109838485717773\n",
            "Epoch [1/5], Step [491/2603], Loss: 3.946614980697632\n",
            "Epoch [1/5], Step [501/2603], Loss: 3.9428372383117676\n",
            "Epoch [1/5], Step [511/2603], Loss: 3.573183298110962\n",
            "Epoch [1/5], Step [521/2603], Loss: 3.6541707515716553\n",
            "Epoch [1/5], Step [531/2603], Loss: 3.747893810272217\n",
            "Epoch [1/5], Step [541/2603], Loss: 3.803711175918579\n",
            "Epoch [1/5], Step [551/2603], Loss: 3.8681557178497314\n",
            "Epoch [1/5], Step [561/2603], Loss: 3.6961162090301514\n",
            "Epoch [1/5], Step [571/2603], Loss: 3.6818416118621826\n",
            "Epoch [1/5], Step [581/2603], Loss: 3.716852903366089\n",
            "Epoch [1/5], Step [591/2603], Loss: 3.610568046569824\n",
            "Epoch [1/5], Step [601/2603], Loss: 3.944183349609375\n",
            "Epoch [1/5], Step [611/2603], Loss: 3.483901023864746\n",
            "Epoch [1/5], Step [621/2603], Loss: 3.616441249847412\n",
            "Epoch [1/5], Step [631/2603], Loss: 3.678959608078003\n",
            "Epoch [1/5], Step [641/2603], Loss: 3.682224988937378\n",
            "Epoch [1/5], Step [651/2603], Loss: 3.8538525104522705\n",
            "Epoch [1/5], Step [661/2603], Loss: 3.583615779876709\n",
            "Epoch [1/5], Step [671/2603], Loss: 3.8859784603118896\n",
            "Epoch [1/5], Step [681/2603], Loss: 3.8513176441192627\n",
            "Epoch [1/5], Step [691/2603], Loss: 3.471303701400757\n",
            "Epoch [1/5], Step [701/2603], Loss: 3.8994009494781494\n",
            "Epoch [1/5], Step [711/2603], Loss: 3.281491279602051\n",
            "Epoch [1/5], Step [721/2603], Loss: 3.548398494720459\n",
            "Epoch [1/5], Step [731/2603], Loss: 3.717972993850708\n",
            "Epoch [1/5], Step [741/2603], Loss: 3.8547205924987793\n",
            "Epoch [1/5], Step [751/2603], Loss: 3.3160336017608643\n",
            "Epoch [1/5], Step [761/2603], Loss: 3.106041193008423\n",
            "Epoch [1/5], Step [771/2603], Loss: 3.396104335784912\n",
            "Epoch [1/5], Step [781/2603], Loss: 3.461127758026123\n",
            "Epoch [1/5], Step [791/2603], Loss: 3.3654651641845703\n",
            "Epoch [1/5], Step [801/2603], Loss: 3.4492313861846924\n",
            "Epoch [1/5], Step [811/2603], Loss: 3.083987236022949\n",
            "Epoch [1/5], Step [821/2603], Loss: 3.7327213287353516\n",
            "Epoch [1/5], Step [831/2603], Loss: 4.004281997680664\n",
            "Epoch [1/5], Step [841/2603], Loss: 3.6235873699188232\n",
            "Epoch [1/5], Step [851/2603], Loss: 3.501535177230835\n",
            "Epoch [1/5], Step [861/2603], Loss: 3.3142454624176025\n",
            "Epoch [1/5], Step [871/2603], Loss: 3.754124402999878\n",
            "Epoch [1/5], Step [881/2603], Loss: 3.0551834106445312\n",
            "Epoch [1/5], Step [891/2603], Loss: 3.71073842048645\n",
            "Epoch [1/5], Step [901/2603], Loss: 3.482085943222046\n",
            "Epoch [1/5], Step [911/2603], Loss: 3.4116973876953125\n",
            "Epoch [1/5], Step [921/2603], Loss: 3.1239326000213623\n",
            "Epoch [1/5], Step [931/2603], Loss: 3.896048069000244\n",
            "Epoch [1/5], Step [941/2603], Loss: 3.0706634521484375\n",
            "Epoch [1/5], Step [951/2603], Loss: 3.1763031482696533\n",
            "Epoch [1/5], Step [961/2603], Loss: 3.043867826461792\n",
            "Epoch [1/5], Step [971/2603], Loss: 3.353867292404175\n",
            "Epoch [1/5], Step [981/2603], Loss: 3.1918952465057373\n",
            "Epoch [1/5], Step [991/2603], Loss: 3.138521194458008\n",
            "Epoch [1/5], Step [1001/2603], Loss: 2.8346495628356934\n",
            "Epoch [1/5], Step [1011/2603], Loss: 3.2986485958099365\n",
            "Epoch [1/5], Step [1021/2603], Loss: 3.199978828430176\n",
            "Epoch [1/5], Step [1031/2603], Loss: 3.33003568649292\n",
            "Epoch [1/5], Step [1041/2603], Loss: 3.2041730880737305\n",
            "Epoch [1/5], Step [1051/2603], Loss: 3.3059113025665283\n",
            "Epoch [1/5], Step [1061/2603], Loss: 3.020141124725342\n",
            "Epoch [1/5], Step [1071/2603], Loss: 3.0242114067077637\n",
            "Epoch [1/5], Step [1081/2603], Loss: 3.0762181282043457\n",
            "Epoch [1/5], Step [1091/2603], Loss: 3.396862745285034\n",
            "Epoch [1/5], Step [1101/2603], Loss: 2.8328208923339844\n",
            "Epoch [1/5], Step [1111/2603], Loss: 3.119884967803955\n",
            "Epoch [1/5], Step [1121/2603], Loss: 3.152864456176758\n",
            "Epoch [1/5], Step [1131/2603], Loss: 3.1075754165649414\n",
            "Epoch [1/5], Step [1141/2603], Loss: 2.9602723121643066\n",
            "Epoch [1/5], Step [1151/2603], Loss: 3.0143821239471436\n",
            "Epoch [1/5], Step [1161/2603], Loss: 3.1574718952178955\n",
            "Epoch [1/5], Step [1171/2603], Loss: 3.03436279296875\n",
            "Epoch [1/5], Step [1181/2603], Loss: 3.377470016479492\n",
            "Epoch [1/5], Step [1191/2603], Loss: 3.045226573944092\n",
            "Epoch [1/5], Step [1201/2603], Loss: 2.987760543823242\n",
            "Epoch [1/5], Step [1211/2603], Loss: 3.1682631969451904\n",
            "Epoch [1/5], Step [1221/2603], Loss: 3.5117053985595703\n",
            "Epoch [1/5], Step [1231/2603], Loss: 3.0834269523620605\n",
            "Epoch [1/5], Step [1241/2603], Loss: 3.247356414794922\n",
            "Epoch [1/5], Step [1251/2603], Loss: 2.90445876121521\n",
            "Epoch [1/5], Step [1261/2603], Loss: 3.0961551666259766\n",
            "Epoch [1/5], Step [1271/2603], Loss: 2.800685405731201\n",
            "Epoch [1/5], Step [1281/2603], Loss: 3.189513683319092\n",
            "Epoch [1/5], Step [1291/2603], Loss: 3.0735509395599365\n",
            "Epoch [1/5], Step [1301/2603], Loss: 3.046618700027466\n",
            "Epoch [1/5], Step [1311/2603], Loss: 3.5704596042633057\n",
            "Epoch [1/5], Step [1321/2603], Loss: 3.094531297683716\n",
            "Epoch [1/5], Step [1331/2603], Loss: 2.9098081588745117\n",
            "Epoch [1/5], Step [1341/2603], Loss: 2.8247127532958984\n",
            "Epoch [1/5], Step [1351/2603], Loss: 3.0596160888671875\n",
            "Epoch [1/5], Step [1361/2603], Loss: 3.2487967014312744\n",
            "Epoch [1/5], Step [1371/2603], Loss: 2.6775572299957275\n",
            "Epoch [1/5], Step [1381/2603], Loss: 2.745528221130371\n",
            "Epoch [1/5], Step [1391/2603], Loss: 2.9277126789093018\n",
            "Epoch [1/5], Step [1401/2603], Loss: 2.7874648571014404\n",
            "Epoch [1/5], Step [1411/2603], Loss: 3.100377321243286\n",
            "Epoch [1/5], Step [1421/2603], Loss: 3.0051019191741943\n",
            "Epoch [1/5], Step [1431/2603], Loss: 2.9181549549102783\n",
            "Epoch [1/5], Step [1441/2603], Loss: 3.055790901184082\n",
            "Epoch [1/5], Step [1451/2603], Loss: 2.8802506923675537\n",
            "Epoch [1/5], Step [1461/2603], Loss: 2.8076634407043457\n",
            "Epoch [1/5], Step [1471/2603], Loss: 2.8420276641845703\n",
            "Epoch [1/5], Step [1481/2603], Loss: 2.816164255142212\n",
            "Epoch [1/5], Step [1491/2603], Loss: 2.946682929992676\n",
            "Epoch [1/5], Step [1501/2603], Loss: 3.0340561866760254\n",
            "Epoch [1/5], Step [1511/2603], Loss: 3.042860746383667\n",
            "Epoch [1/5], Step [1521/2603], Loss: 3.1825337409973145\n",
            "Epoch [1/5], Step [1531/2603], Loss: 2.9416685104370117\n",
            "Epoch [1/5], Step [1541/2603], Loss: 3.1531057357788086\n",
            "Epoch [1/5], Step [1551/2603], Loss: 2.4219553470611572\n",
            "Epoch [1/5], Step [1561/2603], Loss: 2.9160144329071045\n",
            "Epoch [1/5], Step [1571/2603], Loss: 2.5062296390533447\n",
            "Epoch [1/5], Step [1581/2603], Loss: 3.252441644668579\n",
            "Epoch [1/5], Step [1591/2603], Loss: 2.864744186401367\n",
            "Epoch [1/5], Step [1601/2603], Loss: 2.768144130706787\n",
            "Epoch [1/5], Step [1611/2603], Loss: 2.758890390396118\n",
            "Epoch [1/5], Step [1621/2603], Loss: 3.4536798000335693\n",
            "Epoch [1/5], Step [1631/2603], Loss: 2.838467836380005\n",
            "Epoch [1/5], Step [1641/2603], Loss: 2.5558831691741943\n",
            "Epoch [1/5], Step [1651/2603], Loss: 2.894382953643799\n",
            "Epoch [1/5], Step [1661/2603], Loss: 2.710484504699707\n",
            "Epoch [1/5], Step [1671/2603], Loss: 3.082429885864258\n",
            "Epoch [1/5], Step [1681/2603], Loss: 3.074126720428467\n",
            "Epoch [1/5], Step [1691/2603], Loss: 2.5939018726348877\n",
            "Epoch [1/5], Step [1701/2603], Loss: 2.936250686645508\n",
            "Epoch [1/5], Step [1711/2603], Loss: 2.799645185470581\n",
            "Epoch [1/5], Step [1721/2603], Loss: 2.9674055576324463\n",
            "Epoch [1/5], Step [1731/2603], Loss: 2.586317300796509\n",
            "Epoch [1/5], Step [1741/2603], Loss: 2.764023542404175\n",
            "Epoch [1/5], Step [1751/2603], Loss: 2.717047929763794\n",
            "Epoch [1/5], Step [1761/2603], Loss: 2.5679714679718018\n",
            "Epoch [1/5], Step [1771/2603], Loss: 2.829021453857422\n",
            "Epoch [1/5], Step [1781/2603], Loss: 3.164719581604004\n",
            "Epoch [1/5], Step [1791/2603], Loss: 2.9718844890594482\n",
            "Epoch [1/5], Step [1801/2603], Loss: 2.59295392036438\n",
            "Epoch [1/5], Step [1811/2603], Loss: 3.189018964767456\n",
            "Epoch [1/5], Step [1821/2603], Loss: 2.886625289916992\n",
            "Epoch [1/5], Step [1831/2603], Loss: 2.9649853706359863\n",
            "Epoch [1/5], Step [1841/2603], Loss: 2.8006274700164795\n",
            "Epoch [1/5], Step [1851/2603], Loss: 2.247204303741455\n",
            "Epoch [1/5], Step [1861/2603], Loss: 2.6861050128936768\n",
            "Epoch [1/5], Step [1871/2603], Loss: 2.5825042724609375\n",
            "Epoch [1/5], Step [1881/2603], Loss: 2.7527008056640625\n",
            "Epoch [1/5], Step [1891/2603], Loss: 2.902095079421997\n",
            "Epoch [1/5], Step [1901/2603], Loss: 2.6361243724823\n",
            "Epoch [1/5], Step [1911/2603], Loss: 2.514846086502075\n",
            "Epoch [1/5], Step [1921/2603], Loss: 2.860564947128296\n",
            "Epoch [1/5], Step [1931/2603], Loss: 2.6900558471679688\n",
            "Epoch [1/5], Step [1941/2603], Loss: 2.8375954627990723\n",
            "Epoch [1/5], Step [1951/2603], Loss: 2.7155845165252686\n",
            "Epoch [1/5], Step [1961/2603], Loss: 2.7401530742645264\n",
            "Epoch [1/5], Step [1971/2603], Loss: 2.443056106567383\n",
            "Epoch [1/5], Step [1981/2603], Loss: 2.4850218296051025\n",
            "Epoch [1/5], Step [1991/2603], Loss: 2.6991684436798096\n",
            "Epoch [1/5], Step [2001/2603], Loss: 3.1059465408325195\n",
            "Epoch [1/5], Step [2011/2603], Loss: 2.575122594833374\n",
            "Epoch [1/5], Step [2021/2603], Loss: 2.6080496311187744\n",
            "Epoch [1/5], Step [2031/2603], Loss: 2.562190532684326\n",
            "Epoch [1/5], Step [2041/2603], Loss: 2.5152509212493896\n",
            "Epoch [1/5], Step [2051/2603], Loss: 2.7404627799987793\n",
            "Epoch [1/5], Step [2061/2603], Loss: 2.7789127826690674\n",
            "Epoch [1/5], Step [2071/2603], Loss: 2.874420166015625\n",
            "Epoch [1/5], Step [2081/2603], Loss: 2.822050094604492\n",
            "Epoch [1/5], Step [2091/2603], Loss: 2.255608081817627\n",
            "Epoch [1/5], Step [2101/2603], Loss: 2.514374256134033\n",
            "Epoch [1/5], Step [2111/2603], Loss: 2.5290722846984863\n",
            "Epoch [1/5], Step [2121/2603], Loss: 3.004655599594116\n",
            "Epoch [1/5], Step [2131/2603], Loss: 3.0509746074676514\n",
            "Epoch [1/5], Step [2141/2603], Loss: 2.255420684814453\n",
            "Epoch [1/5], Step [2151/2603], Loss: 2.920974016189575\n",
            "Epoch [1/5], Step [2161/2603], Loss: 2.3983330726623535\n",
            "Epoch [1/5], Step [2171/2603], Loss: 2.4554474353790283\n",
            "Epoch [1/5], Step [2181/2603], Loss: 2.8229153156280518\n",
            "Epoch [1/5], Step [2191/2603], Loss: 2.5572898387908936\n",
            "Epoch [1/5], Step [2201/2603], Loss: 2.8577568531036377\n",
            "Epoch [1/5], Step [2211/2603], Loss: 2.704213857650757\n",
            "Epoch [1/5], Step [2221/2603], Loss: 3.263665199279785\n",
            "Epoch [1/5], Step [2231/2603], Loss: 2.7144265174865723\n",
            "Epoch [1/5], Step [2241/2603], Loss: 2.686434745788574\n",
            "Epoch [1/5], Step [2251/2603], Loss: 2.4119887351989746\n",
            "Epoch [1/5], Step [2261/2603], Loss: 2.728043556213379\n",
            "Epoch [1/5], Step [2271/2603], Loss: 2.937159776687622\n",
            "Epoch [1/5], Step [2281/2603], Loss: 2.475592851638794\n",
            "Epoch [1/5], Step [2291/2603], Loss: 2.74180006980896\n",
            "Epoch [1/5], Step [2301/2603], Loss: 2.2674570083618164\n",
            "Epoch [1/5], Step [2311/2603], Loss: 2.732112407684326\n",
            "Epoch [1/5], Step [2321/2603], Loss: 2.4599368572235107\n",
            "Epoch [1/5], Step [2331/2603], Loss: 2.0720722675323486\n",
            "Epoch [1/5], Step [2341/2603], Loss: 2.303422212600708\n",
            "Epoch [1/5], Step [2351/2603], Loss: 2.838982343673706\n",
            "Epoch [1/5], Step [2361/2603], Loss: 2.7553791999816895\n",
            "Epoch [1/5], Step [2371/2603], Loss: 3.0316264629364014\n",
            "Epoch [1/5], Step [2381/2603], Loss: 2.2454705238342285\n",
            "Epoch [1/5], Step [2391/2603], Loss: 2.585768938064575\n",
            "Epoch [1/5], Step [2401/2603], Loss: 2.5033934116363525\n",
            "Epoch [1/5], Step [2411/2603], Loss: 2.4477455615997314\n",
            "Epoch [1/5], Step [2421/2603], Loss: 2.620042324066162\n",
            "Epoch [1/5], Step [2431/2603], Loss: 2.5319442749023438\n",
            "Epoch [1/5], Step [2441/2603], Loss: 2.1147115230560303\n",
            "Epoch [1/5], Step [2451/2603], Loss: 2.4336600303649902\n",
            "Epoch [1/5], Step [2461/2603], Loss: 2.268815279006958\n",
            "Epoch [1/5], Step [2471/2603], Loss: 2.6085681915283203\n",
            "Epoch [1/5], Step [2481/2603], Loss: 2.8382134437561035\n",
            "Epoch [1/5], Step [2491/2603], Loss: 2.2955007553100586\n",
            "Epoch [1/5], Step [2501/2603], Loss: 2.801605701446533\n",
            "Epoch [1/5], Step [2511/2603], Loss: 2.4559664726257324\n",
            "Epoch [1/5], Step [2521/2603], Loss: 2.306044578552246\n",
            "Epoch [1/5], Step [2531/2603], Loss: 2.2356960773468018\n",
            "Epoch [1/5], Step [2541/2603], Loss: 2.1810104846954346\n",
            "Epoch [1/5], Step [2551/2603], Loss: 2.528654098510742\n",
            "Epoch [1/5], Step [2561/2603], Loss: 2.4416897296905518\n",
            "Epoch [1/5], Step [2571/2603], Loss: 2.7614190578460693\n",
            "Epoch [1/5], Step [2581/2603], Loss: 2.6551413536071777\n",
            "Epoch [1/5], Step [2591/2603], Loss: 2.5691778659820557\n",
            "Epoch [1/5], Step [2601/2603], Loss: 2.044964551925659\n",
            "Epoch [2/5], Step [1/2603], Loss: 1.9595144987106323\n",
            "Epoch [2/5], Step [11/2603], Loss: 2.1907505989074707\n",
            "Epoch [2/5], Step [21/2603], Loss: 2.101651430130005\n",
            "Epoch [2/5], Step [31/2603], Loss: 2.1516106128692627\n",
            "Epoch [2/5], Step [41/2603], Loss: 1.8708175420761108\n",
            "Epoch [2/5], Step [51/2603], Loss: 2.173168897628784\n",
            "Epoch [2/5], Step [61/2603], Loss: 2.298048734664917\n",
            "Epoch [2/5], Step [71/2603], Loss: 1.6120582818984985\n",
            "Epoch [2/5], Step [81/2603], Loss: 1.8100790977478027\n",
            "Epoch [2/5], Step [91/2603], Loss: 2.3370609283447266\n",
            "Epoch [2/5], Step [101/2603], Loss: 2.271960973739624\n",
            "Epoch [2/5], Step [111/2603], Loss: 2.4730069637298584\n",
            "Epoch [2/5], Step [121/2603], Loss: 2.3951847553253174\n",
            "Epoch [2/5], Step [131/2603], Loss: 2.42480731010437\n",
            "Epoch [2/5], Step [141/2603], Loss: 2.017916679382324\n",
            "Epoch [2/5], Step [151/2603], Loss: 1.8520681858062744\n",
            "Epoch [2/5], Step [161/2603], Loss: 2.3334569931030273\n",
            "Epoch [2/5], Step [171/2603], Loss: 1.9425439834594727\n",
            "Epoch [2/5], Step [181/2603], Loss: 2.38175106048584\n",
            "Epoch [2/5], Step [191/2603], Loss: 2.080404043197632\n",
            "Epoch [2/5], Step [201/2603], Loss: 1.623233437538147\n",
            "Epoch [2/5], Step [211/2603], Loss: 1.8929003477096558\n",
            "Epoch [2/5], Step [221/2603], Loss: 1.6536840200424194\n",
            "Epoch [2/5], Step [231/2603], Loss: 2.41344952583313\n",
            "Epoch [2/5], Step [241/2603], Loss: 1.922885775566101\n",
            "Epoch [2/5], Step [251/2603], Loss: 2.262540817260742\n",
            "Epoch [2/5], Step [261/2603], Loss: 1.8293836116790771\n",
            "Epoch [2/5], Step [271/2603], Loss: 1.9543957710266113\n",
            "Epoch [2/5], Step [281/2603], Loss: 1.835922360420227\n",
            "Epoch [2/5], Step [291/2603], Loss: 1.8489609956741333\n",
            "Epoch [2/5], Step [301/2603], Loss: 2.152333974838257\n",
            "Epoch [2/5], Step [311/2603], Loss: 2.114257335662842\n",
            "Epoch [2/5], Step [321/2603], Loss: 2.0138561725616455\n",
            "Epoch [2/5], Step [331/2603], Loss: 2.037128210067749\n",
            "Epoch [2/5], Step [341/2603], Loss: 1.9175126552581787\n",
            "Epoch [2/5], Step [351/2603], Loss: 2.277604103088379\n",
            "Epoch [2/5], Step [361/2603], Loss: 1.9159531593322754\n",
            "Epoch [2/5], Step [371/2603], Loss: 1.8377422094345093\n",
            "Epoch [2/5], Step [381/2603], Loss: 2.2795591354370117\n",
            "Epoch [2/5], Step [391/2603], Loss: 1.9222691059112549\n",
            "Epoch [2/5], Step [401/2603], Loss: 2.069939374923706\n",
            "Epoch [2/5], Step [411/2603], Loss: 1.7976007461547852\n",
            "Epoch [2/5], Step [421/2603], Loss: 2.176543951034546\n",
            "Epoch [2/5], Step [431/2603], Loss: 2.1706953048706055\n",
            "Epoch [2/5], Step [441/2603], Loss: 2.187483310699463\n",
            "Epoch [2/5], Step [451/2603], Loss: 2.039018392562866\n",
            "Epoch [2/5], Step [461/2603], Loss: 2.19050669670105\n",
            "Epoch [2/5], Step [471/2603], Loss: 2.3416566848754883\n",
            "Epoch [2/5], Step [481/2603], Loss: 1.943392038345337\n",
            "Epoch [2/5], Step [491/2603], Loss: 1.8647727966308594\n",
            "Epoch [2/5], Step [501/2603], Loss: 1.8518930673599243\n",
            "Epoch [2/5], Step [511/2603], Loss: 2.0791800022125244\n",
            "Epoch [2/5], Step [521/2603], Loss: 1.7507001161575317\n",
            "Epoch [2/5], Step [531/2603], Loss: 1.9019702672958374\n",
            "Epoch [2/5], Step [541/2603], Loss: 2.369880199432373\n",
            "Epoch [2/5], Step [551/2603], Loss: 2.2002005577087402\n",
            "Epoch [2/5], Step [561/2603], Loss: 1.9713188409805298\n",
            "Epoch [2/5], Step [571/2603], Loss: 1.8140919208526611\n",
            "Epoch [2/5], Step [581/2603], Loss: 2.3924405574798584\n",
            "Epoch [2/5], Step [591/2603], Loss: 1.9518452882766724\n",
            "Epoch [2/5], Step [601/2603], Loss: 1.8367117643356323\n",
            "Epoch [2/5], Step [611/2603], Loss: 2.0333099365234375\n",
            "Epoch [2/5], Step [621/2603], Loss: 2.0277771949768066\n",
            "Epoch [2/5], Step [631/2603], Loss: 1.9370677471160889\n",
            "Epoch [2/5], Step [641/2603], Loss: 1.975886583328247\n",
            "Epoch [2/5], Step [651/2603], Loss: 1.8825275897979736\n",
            "Epoch [2/5], Step [661/2603], Loss: 2.2678794860839844\n",
            "Epoch [2/5], Step [671/2603], Loss: 2.1752378940582275\n",
            "Epoch [2/5], Step [681/2603], Loss: 1.952619194984436\n",
            "Epoch [2/5], Step [691/2603], Loss: 1.6199501752853394\n",
            "Epoch [2/5], Step [701/2603], Loss: 1.8671492338180542\n",
            "Epoch [2/5], Step [711/2603], Loss: 2.016364336013794\n",
            "Epoch [2/5], Step [721/2603], Loss: 1.9020099639892578\n",
            "Epoch [2/5], Step [731/2603], Loss: 1.9910246133804321\n",
            "Epoch [2/5], Step [741/2603], Loss: 1.912519931793213\n",
            "Epoch [2/5], Step [751/2603], Loss: 1.8060024976730347\n",
            "Epoch [2/5], Step [761/2603], Loss: 1.6698647737503052\n",
            "Epoch [2/5], Step [771/2603], Loss: 2.042935371398926\n",
            "Epoch [2/5], Step [781/2603], Loss: 1.8481096029281616\n",
            "Epoch [2/5], Step [791/2603], Loss: 1.909996509552002\n",
            "Epoch [2/5], Step [801/2603], Loss: 2.10847806930542\n",
            "Epoch [2/5], Step [811/2603], Loss: 2.04504132270813\n",
            "Epoch [2/5], Step [821/2603], Loss: 1.795857310295105\n",
            "Epoch [2/5], Step [831/2603], Loss: 1.9947260618209839\n",
            "Epoch [2/5], Step [841/2603], Loss: 2.1131808757781982\n",
            "Epoch [2/5], Step [851/2603], Loss: 1.9076218605041504\n",
            "Epoch [2/5], Step [861/2603], Loss: 1.9681293964385986\n",
            "Epoch [2/5], Step [871/2603], Loss: 1.9808316230773926\n",
            "Epoch [2/5], Step [881/2603], Loss: 1.9149608612060547\n",
            "Epoch [2/5], Step [891/2603], Loss: 1.7992041110992432\n",
            "Epoch [2/5], Step [901/2603], Loss: 1.8698999881744385\n",
            "Epoch [2/5], Step [911/2603], Loss: 2.156399965286255\n",
            "Epoch [2/5], Step [921/2603], Loss: 2.0887341499328613\n",
            "Epoch [2/5], Step [931/2603], Loss: 2.003619432449341\n",
            "Epoch [2/5], Step [941/2603], Loss: 1.9874080419540405\n",
            "Epoch [2/5], Step [951/2603], Loss: 1.9919638633728027\n",
            "Epoch [2/5], Step [961/2603], Loss: 1.7007018327713013\n",
            "Epoch [2/5], Step [971/2603], Loss: 2.3695085048675537\n",
            "Epoch [2/5], Step [981/2603], Loss: 1.8751640319824219\n",
            "Epoch [2/5], Step [991/2603], Loss: 2.195842981338501\n",
            "Epoch [2/5], Step [1001/2603], Loss: 2.0734498500823975\n",
            "Epoch [2/5], Step [1011/2603], Loss: 1.94496750831604\n",
            "Epoch [2/5], Step [1021/2603], Loss: 2.4747581481933594\n",
            "Epoch [2/5], Step [1031/2603], Loss: 2.058274030685425\n",
            "Epoch [2/5], Step [1041/2603], Loss: 1.904528021812439\n",
            "Epoch [2/5], Step [1051/2603], Loss: 1.7711708545684814\n",
            "Epoch [2/5], Step [1061/2603], Loss: 1.8989262580871582\n",
            "Epoch [2/5], Step [1071/2603], Loss: 2.090291976928711\n",
            "Epoch [2/5], Step [1081/2603], Loss: 1.8244953155517578\n",
            "Epoch [2/5], Step [1091/2603], Loss: 2.221097230911255\n",
            "Epoch [2/5], Step [1101/2603], Loss: 1.7313592433929443\n",
            "Epoch [2/5], Step [1111/2603], Loss: 1.7097009420394897\n",
            "Epoch [2/5], Step [1121/2603], Loss: 1.8977365493774414\n",
            "Epoch [2/5], Step [1131/2603], Loss: 2.371030807495117\n",
            "Epoch [2/5], Step [1141/2603], Loss: 2.018742799758911\n",
            "Epoch [2/5], Step [1151/2603], Loss: 1.828637957572937\n",
            "Epoch [2/5], Step [1161/2603], Loss: 1.9661842584609985\n",
            "Epoch [2/5], Step [1171/2603], Loss: 2.090134859085083\n",
            "Epoch [2/5], Step [1181/2603], Loss: 2.175938367843628\n",
            "Epoch [2/5], Step [1191/2603], Loss: 1.9059797525405884\n",
            "Epoch [2/5], Step [1201/2603], Loss: 2.080152988433838\n",
            "Epoch [2/5], Step [1211/2603], Loss: 2.34938383102417\n",
            "Epoch [2/5], Step [1221/2603], Loss: 1.9375234842300415\n",
            "Epoch [2/5], Step [1231/2603], Loss: 1.6191251277923584\n",
            "Epoch [2/5], Step [1241/2603], Loss: 2.1240556240081787\n",
            "Epoch [2/5], Step [1251/2603], Loss: 1.8895204067230225\n",
            "Epoch [2/5], Step [1261/2603], Loss: 1.8727797269821167\n",
            "Epoch [2/5], Step [1271/2603], Loss: 1.783473253250122\n",
            "Epoch [2/5], Step [1281/2603], Loss: 1.9370280504226685\n",
            "Epoch [2/5], Step [1291/2603], Loss: 2.036289691925049\n",
            "Epoch [2/5], Step [1301/2603], Loss: 2.085225820541382\n",
            "Epoch [2/5], Step [1311/2603], Loss: 1.9019280672073364\n",
            "Epoch [2/5], Step [1321/2603], Loss: 1.9453885555267334\n",
            "Epoch [2/5], Step [1331/2603], Loss: 1.7227379083633423\n",
            "Epoch [2/5], Step [1341/2603], Loss: 1.6844218969345093\n",
            "Epoch [2/5], Step [1351/2603], Loss: 1.7346889972686768\n",
            "Epoch [2/5], Step [1361/2603], Loss: 2.06817626953125\n",
            "Epoch [2/5], Step [1371/2603], Loss: 2.1438300609588623\n",
            "Epoch [2/5], Step [1381/2603], Loss: 2.109752893447876\n",
            "Epoch [2/5], Step [1391/2603], Loss: 1.5820045471191406\n",
            "Epoch [2/5], Step [1401/2603], Loss: 1.8278470039367676\n",
            "Epoch [2/5], Step [1411/2603], Loss: 2.1482341289520264\n",
            "Epoch [2/5], Step [1421/2603], Loss: 2.1427001953125\n",
            "Epoch [2/5], Step [1431/2603], Loss: 2.1834511756896973\n",
            "Epoch [2/5], Step [1441/2603], Loss: 1.9473942518234253\n",
            "Epoch [2/5], Step [1451/2603], Loss: 1.9947947263717651\n",
            "Epoch [2/5], Step [1461/2603], Loss: 1.8011757135391235\n",
            "Epoch [2/5], Step [1471/2603], Loss: 1.7738139629364014\n",
            "Epoch [2/5], Step [1481/2603], Loss: 1.7024292945861816\n",
            "Epoch [2/5], Step [1491/2603], Loss: 1.8854609727859497\n",
            "Epoch [2/5], Step [1501/2603], Loss: 1.908483862876892\n",
            "Epoch [2/5], Step [1511/2603], Loss: 2.085624933242798\n",
            "Epoch [2/5], Step [1521/2603], Loss: 2.0748519897460938\n",
            "Epoch [2/5], Step [1531/2603], Loss: 1.6562556028366089\n",
            "Epoch [2/5], Step [1541/2603], Loss: 1.94940185546875\n",
            "Epoch [2/5], Step [1551/2603], Loss: 1.629765272140503\n",
            "Epoch [2/5], Step [1561/2603], Loss: 1.8933794498443604\n",
            "Epoch [2/5], Step [1571/2603], Loss: 1.97611665725708\n",
            "Epoch [2/5], Step [1581/2603], Loss: 1.7490534782409668\n",
            "Epoch [2/5], Step [1591/2603], Loss: 2.2322089672088623\n",
            "Epoch [2/5], Step [1601/2603], Loss: 1.3773218393325806\n",
            "Epoch [2/5], Step [1611/2603], Loss: 1.8731536865234375\n",
            "Epoch [2/5], Step [1621/2603], Loss: 2.0554497241973877\n",
            "Epoch [2/5], Step [1631/2603], Loss: 1.791215419769287\n",
            "Epoch [2/5], Step [1641/2603], Loss: 1.8800488710403442\n",
            "Epoch [2/5], Step [1651/2603], Loss: 1.8928569555282593\n",
            "Epoch [2/5], Step [1661/2603], Loss: 1.8092472553253174\n",
            "Epoch [2/5], Step [1671/2603], Loss: 1.8945132493972778\n",
            "Epoch [2/5], Step [1681/2603], Loss: 1.6636844873428345\n",
            "Epoch [2/5], Step [1691/2603], Loss: 1.803712010383606\n",
            "Epoch [2/5], Step [1701/2603], Loss: 2.082014560699463\n",
            "Epoch [2/5], Step [1711/2603], Loss: 1.6262739896774292\n",
            "Epoch [2/5], Step [1721/2603], Loss: 1.9328572750091553\n",
            "Epoch [2/5], Step [1731/2603], Loss: 2.0344133377075195\n",
            "Epoch [2/5], Step [1741/2603], Loss: 1.851824402809143\n",
            "Epoch [2/5], Step [1751/2603], Loss: 1.8900998830795288\n",
            "Epoch [2/5], Step [1761/2603], Loss: 1.8098934888839722\n",
            "Epoch [2/5], Step [1771/2603], Loss: 1.9030215740203857\n",
            "Epoch [2/5], Step [1781/2603], Loss: 1.664084792137146\n",
            "Epoch [2/5], Step [1791/2603], Loss: 1.7704994678497314\n",
            "Epoch [2/5], Step [1801/2603], Loss: 1.9857152700424194\n",
            "Epoch [2/5], Step [1811/2603], Loss: 1.9667493104934692\n",
            "Epoch [2/5], Step [1821/2603], Loss: 1.762189269065857\n",
            "Epoch [2/5], Step [1831/2603], Loss: 2.0715372562408447\n",
            "Epoch [2/5], Step [1841/2603], Loss: 1.620552897453308\n",
            "Epoch [2/5], Step [1851/2603], Loss: 1.634413719177246\n",
            "Epoch [2/5], Step [1861/2603], Loss: 1.7575241327285767\n",
            "Epoch [2/5], Step [1871/2603], Loss: 1.6924145221710205\n",
            "Epoch [2/5], Step [1881/2603], Loss: 1.8040928840637207\n",
            "Epoch [2/5], Step [1891/2603], Loss: 1.8418867588043213\n",
            "Epoch [2/5], Step [1901/2603], Loss: 1.6820008754730225\n",
            "Epoch [2/5], Step [1911/2603], Loss: 1.8276269435882568\n",
            "Epoch [2/5], Step [1921/2603], Loss: 1.6070563793182373\n",
            "Epoch [2/5], Step [1931/2603], Loss: 1.8664666414260864\n",
            "Epoch [2/5], Step [1941/2603], Loss: 1.5768239498138428\n",
            "Epoch [2/5], Step [1951/2603], Loss: 1.8753257989883423\n",
            "Epoch [2/5], Step [1961/2603], Loss: 1.7595717906951904\n",
            "Epoch [2/5], Step [1971/2603], Loss: 1.957870364189148\n",
            "Epoch [2/5], Step [1981/2603], Loss: 1.6126902103424072\n",
            "Epoch [2/5], Step [1991/2603], Loss: 2.007589817047119\n",
            "Epoch [2/5], Step [2001/2603], Loss: 1.7978748083114624\n",
            "Epoch [2/5], Step [2011/2603], Loss: 1.8836121559143066\n",
            "Epoch [2/5], Step [2021/2603], Loss: 1.9606353044509888\n",
            "Epoch [2/5], Step [2031/2603], Loss: 1.9158072471618652\n",
            "Epoch [2/5], Step [2041/2603], Loss: 1.9776220321655273\n",
            "Epoch [2/5], Step [2051/2603], Loss: 1.7110999822616577\n",
            "Epoch [2/5], Step [2061/2603], Loss: 1.7053858041763306\n",
            "Epoch [2/5], Step [2071/2603], Loss: 1.6644967794418335\n",
            "Epoch [2/5], Step [2081/2603], Loss: 1.7422195672988892\n",
            "Epoch [2/5], Step [2091/2603], Loss: 1.2999374866485596\n",
            "Epoch [2/5], Step [2101/2603], Loss: 1.7558873891830444\n",
            "Epoch [2/5], Step [2111/2603], Loss: 1.6429839134216309\n",
            "Epoch [2/5], Step [2121/2603], Loss: 1.5940722227096558\n",
            "Epoch [2/5], Step [2131/2603], Loss: 1.773817539215088\n",
            "Epoch [2/5], Step [2141/2603], Loss: 1.8147108554840088\n",
            "Epoch [2/5], Step [2151/2603], Loss: 2.279076337814331\n",
            "Epoch [2/5], Step [2161/2603], Loss: 2.002758741378784\n",
            "Epoch [2/5], Step [2171/2603], Loss: 1.962624192237854\n",
            "Epoch [2/5], Step [2181/2603], Loss: 1.7431038618087769\n",
            "Epoch [2/5], Step [2191/2603], Loss: 1.7444260120391846\n",
            "Epoch [2/5], Step [2201/2603], Loss: 1.739654779434204\n",
            "Epoch [2/5], Step [2211/2603], Loss: 1.8403695821762085\n",
            "Epoch [2/5], Step [2221/2603], Loss: 1.7794153690338135\n",
            "Epoch [2/5], Step [2231/2603], Loss: 1.6185435056686401\n",
            "Epoch [2/5], Step [2241/2603], Loss: 1.951474666595459\n",
            "Epoch [2/5], Step [2251/2603], Loss: 1.5600333213806152\n",
            "Epoch [2/5], Step [2261/2603], Loss: 1.7285289764404297\n",
            "Epoch [2/5], Step [2271/2603], Loss: 1.7828446626663208\n",
            "Epoch [2/5], Step [2281/2603], Loss: 1.8350685834884644\n",
            "Epoch [2/5], Step [2291/2603], Loss: 1.818044900894165\n",
            "Epoch [2/5], Step [2301/2603], Loss: 1.5161935091018677\n",
            "Epoch [2/5], Step [2311/2603], Loss: 1.9080380201339722\n",
            "Epoch [2/5], Step [2321/2603], Loss: 1.9291050434112549\n",
            "Epoch [2/5], Step [2331/2603], Loss: 2.197291851043701\n",
            "Epoch [2/5], Step [2341/2603], Loss: 2.337739944458008\n",
            "Epoch [2/5], Step [2351/2603], Loss: 1.957833170890808\n",
            "Epoch [2/5], Step [2361/2603], Loss: 1.6674691438674927\n",
            "Epoch [2/5], Step [2371/2603], Loss: 1.5212868452072144\n",
            "Epoch [2/5], Step [2381/2603], Loss: 1.8463729619979858\n",
            "Epoch [2/5], Step [2391/2603], Loss: 2.0304815769195557\n",
            "Epoch [2/5], Step [2401/2603], Loss: 2.091890811920166\n",
            "Epoch [2/5], Step [2411/2603], Loss: 1.922445297241211\n",
            "Epoch [2/5], Step [2421/2603], Loss: 1.96817946434021\n",
            "Epoch [2/5], Step [2431/2603], Loss: 1.688660740852356\n",
            "Epoch [2/5], Step [2441/2603], Loss: 1.8131011724472046\n",
            "Epoch [2/5], Step [2451/2603], Loss: 1.8879265785217285\n",
            "Epoch [2/5], Step [2461/2603], Loss: 2.0430550575256348\n",
            "Epoch [2/5], Step [2471/2603], Loss: 1.6336991786956787\n",
            "Epoch [2/5], Step [2481/2603], Loss: 1.6815812587738037\n",
            "Epoch [2/5], Step [2491/2603], Loss: 1.8452049493789673\n",
            "Epoch [2/5], Step [2501/2603], Loss: 1.836954951286316\n",
            "Epoch [2/5], Step [2511/2603], Loss: 1.7034778594970703\n",
            "Epoch [2/5], Step [2521/2603], Loss: 1.6721781492233276\n",
            "Epoch [2/5], Step [2531/2603], Loss: 1.7393720149993896\n",
            "Epoch [2/5], Step [2541/2603], Loss: 1.7457000017166138\n",
            "Epoch [2/5], Step [2551/2603], Loss: 2.085486650466919\n",
            "Epoch [2/5], Step [2561/2603], Loss: 2.0360405445098877\n",
            "Epoch [2/5], Step [2571/2603], Loss: 1.861714482307434\n",
            "Epoch [2/5], Step [2581/2603], Loss: 2.0987653732299805\n",
            "Epoch [2/5], Step [2591/2603], Loss: 1.716517448425293\n",
            "Epoch [2/5], Step [2601/2603], Loss: 1.940712809562683\n",
            "Epoch [3/5], Step [1/2603], Loss: 1.559193730354309\n",
            "Epoch [3/5], Step [11/2603], Loss: 1.2651110887527466\n",
            "Epoch [3/5], Step [21/2603], Loss: 1.233393669128418\n",
            "Epoch [3/5], Step [31/2603], Loss: 1.4644349813461304\n",
            "Epoch [3/5], Step [41/2603], Loss: 1.4764964580535889\n",
            "Epoch [3/5], Step [51/2603], Loss: 1.2224793434143066\n",
            "Epoch [3/5], Step [61/2603], Loss: 1.3632001876831055\n",
            "Epoch [3/5], Step [71/2603], Loss: 1.5325301885604858\n",
            "Epoch [3/5], Step [81/2603], Loss: 1.3835124969482422\n",
            "Epoch [3/5], Step [91/2603], Loss: 1.643918514251709\n",
            "Epoch [3/5], Step [101/2603], Loss: 1.3787868022918701\n",
            "Epoch [3/5], Step [111/2603], Loss: 1.0700175762176514\n",
            "Epoch [3/5], Step [121/2603], Loss: 1.4621213674545288\n",
            "Epoch [3/5], Step [131/2603], Loss: 1.6658995151519775\n",
            "Epoch [3/5], Step [141/2603], Loss: 1.3080557584762573\n",
            "Epoch [3/5], Step [151/2603], Loss: 0.9596644639968872\n",
            "Epoch [3/5], Step [161/2603], Loss: 1.2627848386764526\n",
            "Epoch [3/5], Step [171/2603], Loss: 1.2652373313903809\n",
            "Epoch [3/5], Step [181/2603], Loss: 1.3070424795150757\n",
            "Epoch [3/5], Step [191/2603], Loss: 1.5446547269821167\n",
            "Epoch [3/5], Step [201/2603], Loss: 1.3990989923477173\n",
            "Epoch [3/5], Step [211/2603], Loss: 1.4322097301483154\n",
            "Epoch [3/5], Step [221/2603], Loss: 1.4514572620391846\n",
            "Epoch [3/5], Step [231/2603], Loss: 1.2472275495529175\n",
            "Epoch [3/5], Step [241/2603], Loss: 1.2401005029678345\n",
            "Epoch [3/5], Step [251/2603], Loss: 1.4088635444641113\n",
            "Epoch [3/5], Step [261/2603], Loss: 1.2354686260223389\n",
            "Epoch [3/5], Step [271/2603], Loss: 1.3587613105773926\n",
            "Epoch [3/5], Step [281/2603], Loss: 1.2918193340301514\n",
            "Epoch [3/5], Step [291/2603], Loss: 1.5286509990692139\n",
            "Epoch [3/5], Step [301/2603], Loss: 1.5716139078140259\n",
            "Epoch [3/5], Step [311/2603], Loss: 1.3893707990646362\n",
            "Epoch [3/5], Step [321/2603], Loss: 1.478529930114746\n",
            "Epoch [3/5], Step [331/2603], Loss: 1.3806416988372803\n",
            "Epoch [3/5], Step [341/2603], Loss: 1.3672239780426025\n",
            "Epoch [3/5], Step [351/2603], Loss: 1.2005763053894043\n",
            "Epoch [3/5], Step [361/2603], Loss: 1.2408047914505005\n",
            "Epoch [3/5], Step [371/2603], Loss: 1.2108138799667358\n",
            "Epoch [3/5], Step [381/2603], Loss: 1.4437668323516846\n",
            "Epoch [3/5], Step [391/2603], Loss: 1.173703670501709\n",
            "Epoch [3/5], Step [401/2603], Loss: 1.4729477167129517\n",
            "Epoch [3/5], Step [411/2603], Loss: 1.019607424736023\n",
            "Epoch [3/5], Step [421/2603], Loss: 1.2819064855575562\n",
            "Epoch [3/5], Step [431/2603], Loss: 1.4769238233566284\n",
            "Epoch [3/5], Step [441/2603], Loss: 1.4390610456466675\n",
            "Epoch [3/5], Step [451/2603], Loss: 1.0547505617141724\n",
            "Epoch [3/5], Step [461/2603], Loss: 1.3153568506240845\n",
            "Epoch [3/5], Step [471/2603], Loss: 1.328148365020752\n",
            "Epoch [3/5], Step [481/2603], Loss: 1.37339448928833\n",
            "Epoch [3/5], Step [491/2603], Loss: 1.3331693410873413\n",
            "Epoch [3/5], Step [501/2603], Loss: 1.563892126083374\n",
            "Epoch [3/5], Step [511/2603], Loss: 1.2647677659988403\n",
            "Epoch [3/5], Step [521/2603], Loss: 1.3997973203659058\n",
            "Epoch [3/5], Step [531/2603], Loss: 1.1673321723937988\n",
            "Epoch [3/5], Step [541/2603], Loss: 1.0745511054992676\n",
            "Epoch [3/5], Step [551/2603], Loss: 1.1405707597732544\n",
            "Epoch [3/5], Step [561/2603], Loss: 1.3529391288757324\n",
            "Epoch [3/5], Step [571/2603], Loss: 1.59463369846344\n",
            "Epoch [3/5], Step [581/2603], Loss: 1.2452998161315918\n",
            "Epoch [3/5], Step [591/2603], Loss: 1.4949904680252075\n",
            "Epoch [3/5], Step [601/2603], Loss: 1.4708324670791626\n",
            "Epoch [3/5], Step [611/2603], Loss: 1.4876301288604736\n",
            "Epoch [3/5], Step [621/2603], Loss: 1.167685866355896\n",
            "Epoch [3/5], Step [631/2603], Loss: 1.4695260524749756\n",
            "Epoch [3/5], Step [641/2603], Loss: 1.3652291297912598\n",
            "Epoch [3/5], Step [651/2603], Loss: 1.2964229583740234\n",
            "Epoch [3/5], Step [661/2603], Loss: 1.458018183708191\n",
            "Epoch [3/5], Step [671/2603], Loss: 1.2618836164474487\n",
            "Epoch [3/5], Step [681/2603], Loss: 1.147286057472229\n",
            "Epoch [3/5], Step [691/2603], Loss: 1.6321052312850952\n",
            "Epoch [3/5], Step [701/2603], Loss: 1.19960618019104\n",
            "Epoch [3/5], Step [711/2603], Loss: 1.200563907623291\n",
            "Epoch [3/5], Step [721/2603], Loss: 1.402943730354309\n",
            "Epoch [3/5], Step [731/2603], Loss: 1.3471819162368774\n",
            "Epoch [3/5], Step [741/2603], Loss: 1.4220349788665771\n",
            "Epoch [3/5], Step [751/2603], Loss: 1.5817184448242188\n",
            "Epoch [3/5], Step [761/2603], Loss: 1.3475221395492554\n",
            "Epoch [3/5], Step [771/2603], Loss: 1.4005296230316162\n",
            "Epoch [3/5], Step [781/2603], Loss: 1.7193788290023804\n",
            "Epoch [3/5], Step [791/2603], Loss: 1.3676941394805908\n",
            "Epoch [3/5], Step [801/2603], Loss: 1.4478024244308472\n",
            "Epoch [3/5], Step [811/2603], Loss: 1.3661445379257202\n",
            "Epoch [3/5], Step [821/2603], Loss: 1.383716106414795\n",
            "Epoch [3/5], Step [831/2603], Loss: 1.4738099575042725\n",
            "Epoch [3/5], Step [841/2603], Loss: 1.5460642576217651\n",
            "Epoch [3/5], Step [851/2603], Loss: 1.0518072843551636\n",
            "Epoch [3/5], Step [861/2603], Loss: 1.2425642013549805\n",
            "Epoch [3/5], Step [871/2603], Loss: 1.4407258033752441\n",
            "Epoch [3/5], Step [881/2603], Loss: 1.4463955163955688\n",
            "Epoch [3/5], Step [891/2603], Loss: 1.3570528030395508\n",
            "Epoch [3/5], Step [901/2603], Loss: 1.2457709312438965\n",
            "Epoch [3/5], Step [911/2603], Loss: 1.2775758504867554\n",
            "Epoch [3/5], Step [921/2603], Loss: 1.552254557609558\n",
            "Epoch [3/5], Step [931/2603], Loss: 1.2273873090744019\n",
            "Epoch [3/5], Step [941/2603], Loss: 1.1371208429336548\n",
            "Epoch [3/5], Step [951/2603], Loss: 1.3304307460784912\n",
            "Epoch [3/5], Step [961/2603], Loss: 1.2813196182250977\n",
            "Epoch [3/5], Step [971/2603], Loss: 1.3010445833206177\n",
            "Epoch [3/5], Step [981/2603], Loss: 1.4914606809616089\n",
            "Epoch [3/5], Step [991/2603], Loss: 1.6206657886505127\n",
            "Epoch [3/5], Step [1001/2603], Loss: 1.255920171737671\n",
            "Epoch [3/5], Step [1011/2603], Loss: 1.3520547151565552\n",
            "Epoch [3/5], Step [1021/2603], Loss: 1.4449292421340942\n",
            "Epoch [3/5], Step [1031/2603], Loss: 1.5043021440505981\n",
            "Epoch [3/5], Step [1041/2603], Loss: 1.4701577425003052\n",
            "Epoch [3/5], Step [1051/2603], Loss: 1.2149134874343872\n",
            "Epoch [3/5], Step [1061/2603], Loss: 1.0663822889328003\n",
            "Epoch [3/5], Step [1071/2603], Loss: 1.1883468627929688\n",
            "Epoch [3/5], Step [1081/2603], Loss: 1.2458841800689697\n",
            "Epoch [3/5], Step [1091/2603], Loss: 1.2957770824432373\n",
            "Epoch [3/5], Step [1101/2603], Loss: 1.2785983085632324\n",
            "Epoch [3/5], Step [1111/2603], Loss: 1.1415467262268066\n",
            "Epoch [3/5], Step [1121/2603], Loss: 1.5191235542297363\n",
            "Epoch [3/5], Step [1131/2603], Loss: 1.241266131401062\n",
            "Epoch [3/5], Step [1141/2603], Loss: 1.503303050994873\n",
            "Epoch [3/5], Step [1151/2603], Loss: 1.2011377811431885\n",
            "Epoch [3/5], Step [1161/2603], Loss: 1.4381393194198608\n",
            "Epoch [3/5], Step [1171/2603], Loss: 1.4669604301452637\n",
            "Epoch [3/5], Step [1181/2603], Loss: 1.3676754236221313\n",
            "Epoch [3/5], Step [1191/2603], Loss: 1.3753595352172852\n",
            "Epoch [3/5], Step [1201/2603], Loss: 1.2515593767166138\n",
            "Epoch [3/5], Step [1211/2603], Loss: 1.3636553287506104\n",
            "Epoch [3/5], Step [1221/2603], Loss: 1.6330596208572388\n",
            "Epoch [3/5], Step [1231/2603], Loss: 1.7497836351394653\n",
            "Epoch [3/5], Step [1241/2603], Loss: 1.5466855764389038\n",
            "Epoch [3/5], Step [1251/2603], Loss: 1.4961472749710083\n",
            "Epoch [3/5], Step [1261/2603], Loss: 1.28825044631958\n",
            "Epoch [3/5], Step [1271/2603], Loss: 1.206742525100708\n",
            "Epoch [3/5], Step [1281/2603], Loss: 1.3385882377624512\n",
            "Epoch [3/5], Step [1291/2603], Loss: 1.1398690938949585\n",
            "Epoch [3/5], Step [1301/2603], Loss: 1.5687066316604614\n",
            "Epoch [3/5], Step [1311/2603], Loss: 1.0899666547775269\n",
            "Epoch [3/5], Step [1321/2603], Loss: 1.5323601961135864\n",
            "Epoch [3/5], Step [1331/2603], Loss: 1.3417216539382935\n",
            "Epoch [3/5], Step [1341/2603], Loss: 1.4945106506347656\n",
            "Epoch [3/5], Step [1351/2603], Loss: 1.2926764488220215\n",
            "Epoch [3/5], Step [1361/2603], Loss: 1.4301323890686035\n",
            "Epoch [3/5], Step [1371/2603], Loss: 1.3526959419250488\n",
            "Epoch [3/5], Step [1381/2603], Loss: 1.3475620746612549\n",
            "Epoch [3/5], Step [1391/2603], Loss: 1.278516173362732\n",
            "Epoch [3/5], Step [1401/2603], Loss: 1.294443130493164\n",
            "Epoch [3/5], Step [1411/2603], Loss: 1.5243598222732544\n",
            "Epoch [3/5], Step [1421/2603], Loss: 1.394126057624817\n",
            "Epoch [3/5], Step [1431/2603], Loss: 1.1624259948730469\n",
            "Epoch [3/5], Step [1441/2603], Loss: 1.4813066720962524\n",
            "Epoch [3/5], Step [1451/2603], Loss: 1.4412720203399658\n",
            "Epoch [3/5], Step [1461/2603], Loss: 1.222681999206543\n",
            "Epoch [3/5], Step [1471/2603], Loss: 1.6570844650268555\n",
            "Epoch [3/5], Step [1481/2603], Loss: 1.2544279098510742\n",
            "Epoch [3/5], Step [1491/2603], Loss: 1.428085207939148\n",
            "Epoch [3/5], Step [1501/2603], Loss: 1.2475041151046753\n",
            "Epoch [3/5], Step [1511/2603], Loss: 1.6154758930206299\n",
            "Epoch [3/5], Step [1521/2603], Loss: 1.6481910943984985\n",
            "Epoch [3/5], Step [1531/2603], Loss: 1.2785338163375854\n",
            "Epoch [3/5], Step [1541/2603], Loss: 1.218554139137268\n",
            "Epoch [3/5], Step [1551/2603], Loss: 1.7908802032470703\n",
            "Epoch [3/5], Step [1561/2603], Loss: 1.3282305002212524\n",
            "Epoch [3/5], Step [1571/2603], Loss: 1.1207125186920166\n",
            "Epoch [3/5], Step [1581/2603], Loss: 1.4281933307647705\n",
            "Epoch [3/5], Step [1591/2603], Loss: 1.3549425601959229\n",
            "Epoch [3/5], Step [1601/2603], Loss: 1.4267640113830566\n",
            "Epoch [3/5], Step [1611/2603], Loss: 1.4213169813156128\n",
            "Epoch [3/5], Step [1621/2603], Loss: 1.190216302871704\n",
            "Epoch [3/5], Step [1631/2603], Loss: 1.3502253293991089\n",
            "Epoch [3/5], Step [1641/2603], Loss: 1.352608323097229\n",
            "Epoch [3/5], Step [1651/2603], Loss: 1.5229027271270752\n",
            "Epoch [3/5], Step [1661/2603], Loss: 1.2202140092849731\n",
            "Epoch [3/5], Step [1671/2603], Loss: 1.4924302101135254\n",
            "Epoch [3/5], Step [1681/2603], Loss: 1.311631441116333\n",
            "Epoch [3/5], Step [1691/2603], Loss: 1.3113764524459839\n",
            "Epoch [3/5], Step [1701/2603], Loss: 1.3533607721328735\n",
            "Epoch [3/5], Step [1711/2603], Loss: 1.3905067443847656\n",
            "Epoch [3/5], Step [1721/2603], Loss: 1.5715148448944092\n",
            "Epoch [3/5], Step [1731/2603], Loss: 1.2282885313034058\n",
            "Epoch [3/5], Step [1741/2603], Loss: 1.2303800582885742\n",
            "Epoch [3/5], Step [1751/2603], Loss: 1.4386345148086548\n",
            "Epoch [3/5], Step [1761/2603], Loss: 1.223182201385498\n",
            "Epoch [3/5], Step [1771/2603], Loss: 1.6121776103973389\n",
            "Epoch [3/5], Step [1781/2603], Loss: 1.0519030094146729\n",
            "Epoch [3/5], Step [1791/2603], Loss: 1.4142009019851685\n",
            "Epoch [3/5], Step [1801/2603], Loss: 1.2370344400405884\n",
            "Epoch [3/5], Step [1811/2603], Loss: 1.2005995512008667\n",
            "Epoch [3/5], Step [1821/2603], Loss: 1.2804226875305176\n",
            "Epoch [3/5], Step [1831/2603], Loss: 1.5780870914459229\n",
            "Epoch [3/5], Step [1841/2603], Loss: 1.0684493780136108\n",
            "Epoch [3/5], Step [1851/2603], Loss: 1.4929864406585693\n",
            "Epoch [3/5], Step [1861/2603], Loss: 1.4747511148452759\n",
            "Epoch [3/5], Step [1871/2603], Loss: 1.1600284576416016\n",
            "Epoch [3/5], Step [1881/2603], Loss: 1.163522720336914\n",
            "Epoch [3/5], Step [1891/2603], Loss: 1.2458651065826416\n",
            "Epoch [3/5], Step [1901/2603], Loss: 1.3104242086410522\n",
            "Epoch [3/5], Step [1911/2603], Loss: 1.5484172105789185\n",
            "Epoch [3/5], Step [1921/2603], Loss: 1.5263725519180298\n",
            "Epoch [3/5], Step [1931/2603], Loss: 1.4670727252960205\n",
            "Epoch [3/5], Step [1941/2603], Loss: 1.5287773609161377\n",
            "Epoch [3/5], Step [1951/2603], Loss: 1.2925039529800415\n",
            "Epoch [3/5], Step [1961/2603], Loss: 1.5635689496994019\n",
            "Epoch [3/5], Step [1971/2603], Loss: 1.388866901397705\n",
            "Epoch [3/5], Step [1981/2603], Loss: 1.6222035884857178\n",
            "Epoch [3/5], Step [1991/2603], Loss: 1.4975823163986206\n",
            "Epoch [3/5], Step [2001/2603], Loss: 1.4855320453643799\n",
            "Epoch [3/5], Step [2011/2603], Loss: 1.2800847291946411\n",
            "Epoch [3/5], Step [2021/2603], Loss: 1.3026819229125977\n",
            "Epoch [3/5], Step [2031/2603], Loss: 1.2884405851364136\n",
            "Epoch [3/5], Step [2041/2603], Loss: 1.233344316482544\n",
            "Epoch [3/5], Step [2051/2603], Loss: 1.4457590579986572\n",
            "Epoch [3/5], Step [2061/2603], Loss: 1.2169932126998901\n",
            "Epoch [3/5], Step [2071/2603], Loss: 1.4363434314727783\n",
            "Epoch [3/5], Step [2081/2603], Loss: 1.079837679862976\n",
            "Epoch [3/5], Step [2091/2603], Loss: 1.1570402383804321\n",
            "Epoch [3/5], Step [2101/2603], Loss: 1.0646460056304932\n",
            "Epoch [3/5], Step [2111/2603], Loss: 1.3541655540466309\n",
            "Epoch [3/5], Step [2121/2603], Loss: 1.1031571626663208\n",
            "Epoch [3/5], Step [2131/2603], Loss: 1.0840471982955933\n",
            "Epoch [3/5], Step [2141/2603], Loss: 1.5586612224578857\n",
            "Epoch [3/5], Step [2151/2603], Loss: 1.2163499593734741\n",
            "Epoch [3/5], Step [2161/2603], Loss: 1.3279056549072266\n",
            "Epoch [3/5], Step [2171/2603], Loss: 1.3622268438339233\n",
            "Epoch [3/5], Step [2181/2603], Loss: 1.180083155632019\n",
            "Epoch [3/5], Step [2191/2603], Loss: 1.4488738775253296\n",
            "Epoch [3/5], Step [2201/2603], Loss: 1.37949538230896\n",
            "Epoch [3/5], Step [2211/2603], Loss: 1.1577812433242798\n",
            "Epoch [3/5], Step [2221/2603], Loss: 1.1120729446411133\n",
            "Epoch [3/5], Step [2231/2603], Loss: 1.3460888862609863\n",
            "Epoch [3/5], Step [2241/2603], Loss: 1.4924399852752686\n",
            "Epoch [3/5], Step [2251/2603], Loss: 1.4364279508590698\n",
            "Epoch [3/5], Step [2261/2603], Loss: 1.435679316520691\n",
            "Epoch [3/5], Step [2271/2603], Loss: 1.2186458110809326\n",
            "Epoch [3/5], Step [2281/2603], Loss: 1.3639073371887207\n",
            "Epoch [3/5], Step [2291/2603], Loss: 1.4707822799682617\n",
            "Epoch [3/5], Step [2301/2603], Loss: 1.249846339225769\n",
            "Epoch [3/5], Step [2311/2603], Loss: 1.4366642236709595\n",
            "Epoch [3/5], Step [2321/2603], Loss: 1.1946555376052856\n",
            "Epoch [3/5], Step [2331/2603], Loss: 1.4393999576568604\n",
            "Epoch [3/5], Step [2341/2603], Loss: 1.3159706592559814\n",
            "Epoch [3/5], Step [2351/2603], Loss: 1.370521903038025\n",
            "Epoch [3/5], Step [2361/2603], Loss: 1.2055919170379639\n",
            "Epoch [3/5], Step [2371/2603], Loss: 1.2534801959991455\n",
            "Epoch [3/5], Step [2381/2603], Loss: 1.2371740341186523\n",
            "Epoch [3/5], Step [2391/2603], Loss: 1.482494592666626\n",
            "Epoch [3/5], Step [2401/2603], Loss: 1.0639619827270508\n",
            "Epoch [3/5], Step [2411/2603], Loss: 1.316240668296814\n",
            "Epoch [3/5], Step [2421/2603], Loss: 1.5322240591049194\n",
            "Epoch [3/5], Step [2431/2603], Loss: 1.217740774154663\n",
            "Epoch [3/5], Step [2441/2603], Loss: 1.3581520318984985\n",
            "Epoch [3/5], Step [2451/2603], Loss: 1.1669344902038574\n",
            "Epoch [3/5], Step [2461/2603], Loss: 1.4587987661361694\n",
            "Epoch [3/5], Step [2471/2603], Loss: 1.361754298210144\n",
            "Epoch [3/5], Step [2481/2603], Loss: 1.241119623184204\n",
            "Epoch [3/5], Step [2491/2603], Loss: 1.2943010330200195\n",
            "Epoch [3/5], Step [2501/2603], Loss: 1.3505079746246338\n",
            "Epoch [3/5], Step [2511/2603], Loss: 1.5793505907058716\n",
            "Epoch [3/5], Step [2521/2603], Loss: 1.228235125541687\n",
            "Epoch [3/5], Step [2531/2603], Loss: 1.06211256980896\n",
            "Epoch [3/5], Step [2541/2603], Loss: 1.4465129375457764\n",
            "Epoch [3/5], Step [2551/2603], Loss: 1.3413348197937012\n",
            "Epoch [3/5], Step [2561/2603], Loss: 1.2205920219421387\n",
            "Epoch [3/5], Step [2571/2603], Loss: 1.2083336114883423\n",
            "Epoch [3/5], Step [2581/2603], Loss: 1.2966322898864746\n",
            "Epoch [3/5], Step [2591/2603], Loss: 1.3978307247161865\n",
            "Epoch [3/5], Step [2601/2603], Loss: 0.9985442757606506\n",
            "Epoch [4/5], Step [1/2603], Loss: 0.9126022458076477\n",
            "Epoch [4/5], Step [11/2603], Loss: 0.7641865611076355\n",
            "Epoch [4/5], Step [21/2603], Loss: 0.9558544158935547\n",
            "Epoch [4/5], Step [31/2603], Loss: 0.8878492116928101\n",
            "Epoch [4/5], Step [41/2603], Loss: 0.8853269815444946\n",
            "Epoch [4/5], Step [51/2603], Loss: 0.7124641537666321\n",
            "Epoch [4/5], Step [61/2603], Loss: 0.7716885209083557\n",
            "Epoch [4/5], Step [71/2603], Loss: 0.8596006631851196\n",
            "Epoch [4/5], Step [81/2603], Loss: 1.0119266510009766\n",
            "Epoch [4/5], Step [91/2603], Loss: 0.9839828610420227\n",
            "Epoch [4/5], Step [101/2603], Loss: 0.9440152049064636\n",
            "Epoch [4/5], Step [111/2603], Loss: 1.0412245988845825\n",
            "Epoch [4/5], Step [121/2603], Loss: 0.7126911878585815\n",
            "Epoch [4/5], Step [131/2603], Loss: 0.8860788345336914\n",
            "Epoch [4/5], Step [141/2603], Loss: 1.068853735923767\n",
            "Epoch [4/5], Step [151/2603], Loss: 0.8058419823646545\n",
            "Epoch [4/5], Step [161/2603], Loss: 0.8333283066749573\n",
            "Epoch [4/5], Step [171/2603], Loss: 1.1151444911956787\n",
            "Epoch [4/5], Step [181/2603], Loss: 0.6754040122032166\n",
            "Epoch [4/5], Step [191/2603], Loss: 0.7615973353385925\n",
            "Epoch [4/5], Step [201/2603], Loss: 0.9901512265205383\n",
            "Epoch [4/5], Step [211/2603], Loss: 0.8912978172302246\n",
            "Epoch [4/5], Step [221/2603], Loss: 0.8131625652313232\n",
            "Epoch [4/5], Step [231/2603], Loss: 1.2541096210479736\n",
            "Epoch [4/5], Step [241/2603], Loss: 0.6913413405418396\n",
            "Epoch [4/5], Step [251/2603], Loss: 0.8657963871955872\n",
            "Epoch [4/5], Step [261/2603], Loss: 0.9868045449256897\n",
            "Epoch [4/5], Step [271/2603], Loss: 0.866523802280426\n",
            "Epoch [4/5], Step [281/2603], Loss: 1.084721565246582\n",
            "Epoch [4/5], Step [291/2603], Loss: 1.0110194683074951\n",
            "Epoch [4/5], Step [301/2603], Loss: 1.1141973733901978\n",
            "Epoch [4/5], Step [311/2603], Loss: 0.9476649165153503\n",
            "Epoch [4/5], Step [321/2603], Loss: 0.9095613360404968\n",
            "Epoch [4/5], Step [331/2603], Loss: 0.8999046087265015\n",
            "Epoch [4/5], Step [341/2603], Loss: 0.9036625027656555\n",
            "Epoch [4/5], Step [351/2603], Loss: 1.1199122667312622\n",
            "Epoch [4/5], Step [361/2603], Loss: 0.7780683040618896\n",
            "Epoch [4/5], Step [371/2603], Loss: 0.6593353152275085\n",
            "Epoch [4/5], Step [381/2603], Loss: 0.9043274521827698\n",
            "Epoch [4/5], Step [391/2603], Loss: 0.9152417778968811\n",
            "Epoch [4/5], Step [401/2603], Loss: 0.8385910391807556\n",
            "Epoch [4/5], Step [411/2603], Loss: 0.8487902283668518\n",
            "Epoch [4/5], Step [421/2603], Loss: 0.854266881942749\n",
            "Epoch [4/5], Step [431/2603], Loss: 0.9113577604293823\n",
            "Epoch [4/5], Step [441/2603], Loss: 0.8494248986244202\n",
            "Epoch [4/5], Step [451/2603], Loss: 0.8217599987983704\n",
            "Epoch [4/5], Step [461/2603], Loss: 0.9559910893440247\n",
            "Epoch [4/5], Step [471/2603], Loss: 0.6796450614929199\n",
            "Epoch [4/5], Step [481/2603], Loss: 0.794075608253479\n",
            "Epoch [4/5], Step [491/2603], Loss: 0.8659517765045166\n",
            "Epoch [4/5], Step [501/2603], Loss: 1.0031172037124634\n",
            "Epoch [4/5], Step [511/2603], Loss: 0.883887529373169\n",
            "Epoch [4/5], Step [521/2603], Loss: 1.2264646291732788\n",
            "Epoch [4/5], Step [531/2603], Loss: 0.8619269728660583\n",
            "Epoch [4/5], Step [541/2603], Loss: 0.7413134574890137\n",
            "Epoch [4/5], Step [551/2603], Loss: 0.8334090709686279\n",
            "Epoch [4/5], Step [561/2603], Loss: 0.8770963549613953\n",
            "Epoch [4/5], Step [571/2603], Loss: 0.9868719577789307\n",
            "Epoch [4/5], Step [581/2603], Loss: 1.025589942932129\n",
            "Epoch [4/5], Step [591/2603], Loss: 0.9499145150184631\n",
            "Epoch [4/5], Step [601/2603], Loss: 0.9917673468589783\n",
            "Epoch [4/5], Step [611/2603], Loss: 0.9182531833648682\n",
            "Epoch [4/5], Step [621/2603], Loss: 0.8955500721931458\n",
            "Epoch [4/5], Step [631/2603], Loss: 1.0522783994674683\n",
            "Epoch [4/5], Step [641/2603], Loss: 0.8532822132110596\n",
            "Epoch [4/5], Step [651/2603], Loss: 0.8477264046669006\n",
            "Epoch [4/5], Step [661/2603], Loss: 1.0340449810028076\n",
            "Epoch [4/5], Step [671/2603], Loss: 1.1189087629318237\n",
            "Epoch [4/5], Step [681/2603], Loss: 0.869135856628418\n",
            "Epoch [4/5], Step [691/2603], Loss: 0.9113415479660034\n",
            "Epoch [4/5], Step [701/2603], Loss: 0.9536256194114685\n",
            "Epoch [4/5], Step [711/2603], Loss: 1.0282071828842163\n",
            "Epoch [4/5], Step [721/2603], Loss: 1.0585278272628784\n",
            "Epoch [4/5], Step [731/2603], Loss: 0.7584549784660339\n",
            "Epoch [4/5], Step [741/2603], Loss: 1.1032817363739014\n",
            "Epoch [4/5], Step [751/2603], Loss: 0.7673419117927551\n",
            "Epoch [4/5], Step [761/2603], Loss: 1.197989583015442\n",
            "Epoch [4/5], Step [771/2603], Loss: 0.9079771637916565\n",
            "Epoch [4/5], Step [781/2603], Loss: 1.0551056861877441\n",
            "Epoch [4/5], Step [791/2603], Loss: 0.921127438545227\n",
            "Epoch [4/5], Step [801/2603], Loss: 1.035245656967163\n",
            "Epoch [4/5], Step [811/2603], Loss: 0.7800877690315247\n",
            "Epoch [4/5], Step [821/2603], Loss: 0.7431151270866394\n",
            "Epoch [4/5], Step [831/2603], Loss: 0.8829547762870789\n",
            "Epoch [4/5], Step [841/2603], Loss: 0.9265474081039429\n",
            "Epoch [4/5], Step [851/2603], Loss: 0.7175853848457336\n",
            "Epoch [4/5], Step [861/2603], Loss: 0.8886012434959412\n",
            "Epoch [4/5], Step [871/2603], Loss: 1.1519749164581299\n",
            "Epoch [4/5], Step [881/2603], Loss: 0.8386530876159668\n",
            "Epoch [4/5], Step [891/2603], Loss: 0.8871749043464661\n",
            "Epoch [4/5], Step [901/2603], Loss: 0.9528843760490417\n",
            "Epoch [4/5], Step [911/2603], Loss: 0.8923026919364929\n",
            "Epoch [4/5], Step [921/2603], Loss: 0.7516026496887207\n",
            "Epoch [4/5], Step [931/2603], Loss: 0.9424942135810852\n",
            "Epoch [4/5], Step [941/2603], Loss: 1.1101223230361938\n",
            "Epoch [4/5], Step [951/2603], Loss: 1.1487046480178833\n",
            "Epoch [4/5], Step [961/2603], Loss: 0.9416868090629578\n",
            "Epoch [4/5], Step [971/2603], Loss: 0.7188330292701721\n",
            "Epoch [4/5], Step [981/2603], Loss: 0.8840195536613464\n",
            "Epoch [4/5], Step [991/2603], Loss: 0.9690322279930115\n",
            "Epoch [4/5], Step [1001/2603], Loss: 1.1369551420211792\n",
            "Epoch [4/5], Step [1011/2603], Loss: 0.8560967445373535\n",
            "Epoch [4/5], Step [1021/2603], Loss: 1.0402801036834717\n",
            "Epoch [4/5], Step [1031/2603], Loss: 0.9088317155838013\n",
            "Epoch [4/5], Step [1041/2603], Loss: 1.0281659364700317\n",
            "Epoch [4/5], Step [1051/2603], Loss: 0.9276299476623535\n",
            "Epoch [4/5], Step [1061/2603], Loss: 0.9020522236824036\n",
            "Epoch [4/5], Step [1071/2603], Loss: 1.0665810108184814\n",
            "Epoch [4/5], Step [1081/2603], Loss: 1.0067036151885986\n",
            "Epoch [4/5], Step [1091/2603], Loss: 1.0071744918823242\n",
            "Epoch [4/5], Step [1101/2603], Loss: 0.8772751688957214\n",
            "Epoch [4/5], Step [1111/2603], Loss: 0.9021844863891602\n",
            "Epoch [4/5], Step [1121/2603], Loss: 0.8049014806747437\n",
            "Epoch [4/5], Step [1131/2603], Loss: 0.8911191821098328\n",
            "Epoch [4/5], Step [1141/2603], Loss: 0.929919421672821\n",
            "Epoch [4/5], Step [1151/2603], Loss: 1.0435383319854736\n",
            "Epoch [4/5], Step [1161/2603], Loss: 0.9077948331832886\n",
            "Epoch [4/5], Step [1171/2603], Loss: 1.080913782119751\n",
            "Epoch [4/5], Step [1181/2603], Loss: 0.9065592885017395\n",
            "Epoch [4/5], Step [1191/2603], Loss: 0.9800273776054382\n",
            "Epoch [4/5], Step [1201/2603], Loss: 0.956013560295105\n",
            "Epoch [4/5], Step [1211/2603], Loss: 0.8234995603561401\n",
            "Epoch [4/5], Step [1221/2603], Loss: 1.022741675376892\n",
            "Epoch [4/5], Step [1231/2603], Loss: 1.102836012840271\n",
            "Epoch [4/5], Step [1241/2603], Loss: 1.01175057888031\n",
            "Epoch [4/5], Step [1251/2603], Loss: 0.8174108862876892\n",
            "Epoch [4/5], Step [1261/2603], Loss: 0.7729352712631226\n",
            "Epoch [4/5], Step [1271/2603], Loss: 0.9013919830322266\n",
            "Epoch [4/5], Step [1281/2603], Loss: 0.8754017949104309\n",
            "Epoch [4/5], Step [1291/2603], Loss: 0.6899462938308716\n",
            "Epoch [4/5], Step [1301/2603], Loss: 0.8988931179046631\n",
            "Epoch [4/5], Step [1311/2603], Loss: 0.9223018884658813\n",
            "Epoch [4/5], Step [1321/2603], Loss: 1.0027687549591064\n",
            "Epoch [4/5], Step [1331/2603], Loss: 1.0043299198150635\n",
            "Epoch [4/5], Step [1341/2603], Loss: 1.0589359998703003\n",
            "Epoch [4/5], Step [1351/2603], Loss: 0.9953958988189697\n",
            "Epoch [4/5], Step [1361/2603], Loss: 0.8810177445411682\n",
            "Epoch [4/5], Step [1371/2603], Loss: 1.2403013706207275\n",
            "Epoch [4/5], Step [1381/2603], Loss: 1.023879885673523\n",
            "Epoch [4/5], Step [1391/2603], Loss: 1.0885430574417114\n",
            "Epoch [4/5], Step [1401/2603], Loss: 0.901563286781311\n",
            "Epoch [4/5], Step [1411/2603], Loss: 0.7723219394683838\n",
            "Epoch [4/5], Step [1421/2603], Loss: 0.8195750713348389\n",
            "Epoch [4/5], Step [1431/2603], Loss: 1.0970425605773926\n",
            "Epoch [4/5], Step [1441/2603], Loss: 1.1264855861663818\n",
            "Epoch [4/5], Step [1451/2603], Loss: 1.1515849828720093\n",
            "Epoch [4/5], Step [1461/2603], Loss: 0.8396735191345215\n",
            "Epoch [4/5], Step [1471/2603], Loss: 0.8575243353843689\n",
            "Epoch [4/5], Step [1481/2603], Loss: 0.9022451639175415\n",
            "Epoch [4/5], Step [1491/2603], Loss: 0.9092288017272949\n",
            "Epoch [4/5], Step [1501/2603], Loss: 0.9582821130752563\n",
            "Epoch [4/5], Step [1511/2603], Loss: 0.9010872840881348\n",
            "Epoch [4/5], Step [1521/2603], Loss: 1.1257894039154053\n",
            "Epoch [4/5], Step [1531/2603], Loss: 0.8202122449874878\n",
            "Epoch [4/5], Step [1541/2603], Loss: 0.8379174470901489\n",
            "Epoch [4/5], Step [1551/2603], Loss: 1.1063973903656006\n",
            "Epoch [4/5], Step [1561/2603], Loss: 1.1514557600021362\n",
            "Epoch [4/5], Step [1571/2603], Loss: 0.6494670510292053\n",
            "Epoch [4/5], Step [1581/2603], Loss: 0.8098122477531433\n",
            "Epoch [4/5], Step [1591/2603], Loss: 0.9808456301689148\n",
            "Epoch [4/5], Step [1601/2603], Loss: 1.0184637308120728\n",
            "Epoch [4/5], Step [1611/2603], Loss: 0.9125325083732605\n",
            "Epoch [4/5], Step [1621/2603], Loss: 1.086154818534851\n",
            "Epoch [4/5], Step [1631/2603], Loss: 1.0340187549591064\n",
            "Epoch [4/5], Step [1641/2603], Loss: 0.935494065284729\n",
            "Epoch [4/5], Step [1651/2603], Loss: 0.9284157752990723\n",
            "Epoch [4/5], Step [1661/2603], Loss: 0.8355081677436829\n",
            "Epoch [4/5], Step [1671/2603], Loss: 0.8536642789840698\n",
            "Epoch [4/5], Step [1681/2603], Loss: 0.8822406530380249\n",
            "Epoch [4/5], Step [1691/2603], Loss: 0.7139940857887268\n",
            "Epoch [4/5], Step [1701/2603], Loss: 0.8253651857376099\n",
            "Epoch [4/5], Step [1711/2603], Loss: 0.921588122844696\n",
            "Epoch [4/5], Step [1721/2603], Loss: 1.183241844177246\n",
            "Epoch [4/5], Step [1731/2603], Loss: 1.029809832572937\n",
            "Epoch [4/5], Step [1741/2603], Loss: 0.7229365110397339\n",
            "Epoch [4/5], Step [1751/2603], Loss: 0.8953751921653748\n",
            "Epoch [4/5], Step [1761/2603], Loss: 1.203028917312622\n",
            "Epoch [4/5], Step [1771/2603], Loss: 1.0308555364608765\n",
            "Epoch [4/5], Step [1781/2603], Loss: 0.9140625596046448\n",
            "Epoch [4/5], Step [1791/2603], Loss: 0.8835378885269165\n",
            "Epoch [4/5], Step [1801/2603], Loss: 0.9712052941322327\n",
            "Epoch [4/5], Step [1811/2603], Loss: 1.075323224067688\n",
            "Epoch [4/5], Step [1821/2603], Loss: 0.9389371275901794\n",
            "Epoch [4/5], Step [1831/2603], Loss: 0.9231910109519958\n",
            "Epoch [4/5], Step [1841/2603], Loss: 0.7810395359992981\n",
            "Epoch [4/5], Step [1851/2603], Loss: 1.1261787414550781\n",
            "Epoch [4/5], Step [1861/2603], Loss: 0.9573467969894409\n",
            "Epoch [4/5], Step [1871/2603], Loss: 1.0556095838546753\n",
            "Epoch [4/5], Step [1881/2603], Loss: 1.0444388389587402\n",
            "Epoch [4/5], Step [1891/2603], Loss: 0.7907498478889465\n",
            "Epoch [4/5], Step [1901/2603], Loss: 0.7593203783035278\n",
            "Epoch [4/5], Step [1911/2603], Loss: 0.9806651473045349\n",
            "Epoch [4/5], Step [1921/2603], Loss: 1.0990229845046997\n",
            "Epoch [4/5], Step [1931/2603], Loss: 0.7969695925712585\n",
            "Epoch [4/5], Step [1941/2603], Loss: 0.8466102480888367\n",
            "Epoch [4/5], Step [1951/2603], Loss: 0.8544036149978638\n",
            "Epoch [4/5], Step [1961/2603], Loss: 0.8465990424156189\n",
            "Epoch [4/5], Step [1971/2603], Loss: 1.2142795324325562\n",
            "Epoch [4/5], Step [1981/2603], Loss: 0.9127910733222961\n",
            "Epoch [4/5], Step [1991/2603], Loss: 0.8443484902381897\n",
            "Epoch [4/5], Step [2001/2603], Loss: 0.9318919777870178\n",
            "Epoch [4/5], Step [2011/2603], Loss: 0.8494424223899841\n",
            "Epoch [4/5], Step [2021/2603], Loss: 0.8058567047119141\n",
            "Epoch [4/5], Step [2031/2603], Loss: 1.079280138015747\n",
            "Epoch [4/5], Step [2041/2603], Loss: 0.8597657680511475\n",
            "Epoch [4/5], Step [2051/2603], Loss: 0.9036406874656677\n",
            "Epoch [4/5], Step [2061/2603], Loss: 0.9465294480323792\n",
            "Epoch [4/5], Step [2071/2603], Loss: 1.1369343996047974\n",
            "Epoch [4/5], Step [2081/2603], Loss: 0.9684176445007324\n",
            "Epoch [4/5], Step [2091/2603], Loss: 1.0135409832000732\n",
            "Epoch [4/5], Step [2101/2603], Loss: 0.967829167842865\n",
            "Epoch [4/5], Step [2111/2603], Loss: 1.3641365766525269\n",
            "Epoch [4/5], Step [2121/2603], Loss: 0.9922078251838684\n",
            "Epoch [4/5], Step [2131/2603], Loss: 0.9411482214927673\n",
            "Epoch [4/5], Step [2141/2603], Loss: 0.8078300952911377\n",
            "Epoch [4/5], Step [2151/2603], Loss: 1.0863184928894043\n",
            "Epoch [4/5], Step [2161/2603], Loss: 0.6896733641624451\n",
            "Epoch [4/5], Step [2171/2603], Loss: 0.8367980718612671\n",
            "Epoch [4/5], Step [2181/2603], Loss: 0.9233770966529846\n",
            "Epoch [4/5], Step [2191/2603], Loss: 1.057286024093628\n",
            "Epoch [4/5], Step [2201/2603], Loss: 0.8848968148231506\n",
            "Epoch [4/5], Step [2211/2603], Loss: 0.7904273867607117\n",
            "Epoch [4/5], Step [2221/2603], Loss: 0.908527135848999\n",
            "Epoch [4/5], Step [2231/2603], Loss: 0.9048874378204346\n",
            "Epoch [4/5], Step [2241/2603], Loss: 0.8823895454406738\n",
            "Epoch [4/5], Step [2251/2603], Loss: 1.0714715719223022\n",
            "Epoch [4/5], Step [2261/2603], Loss: 0.9604659676551819\n",
            "Epoch [4/5], Step [2271/2603], Loss: 1.0431569814682007\n",
            "Epoch [4/5], Step [2281/2603], Loss: 0.9908139109611511\n",
            "Epoch [4/5], Step [2291/2603], Loss: 0.9517597556114197\n",
            "Epoch [4/5], Step [2301/2603], Loss: 0.7973366975784302\n",
            "Epoch [4/5], Step [2311/2603], Loss: 1.0109851360321045\n",
            "Epoch [4/5], Step [2321/2603], Loss: 0.6952270865440369\n",
            "Epoch [4/5], Step [2331/2603], Loss: 0.7980940937995911\n",
            "Epoch [4/5], Step [2341/2603], Loss: 1.2868715524673462\n",
            "Epoch [4/5], Step [2351/2603], Loss: 0.8744761347770691\n",
            "Epoch [4/5], Step [2361/2603], Loss: 0.8363494873046875\n",
            "Epoch [4/5], Step [2371/2603], Loss: 1.019583821296692\n",
            "Epoch [4/5], Step [2381/2603], Loss: 0.9106835722923279\n",
            "Epoch [4/5], Step [2391/2603], Loss: 1.0835074186325073\n",
            "Epoch [4/5], Step [2401/2603], Loss: 1.0746352672576904\n",
            "Epoch [4/5], Step [2411/2603], Loss: 0.8544548153877258\n",
            "Epoch [4/5], Step [2421/2603], Loss: 0.6964185237884521\n",
            "Epoch [4/5], Step [2431/2603], Loss: 0.7935922741889954\n",
            "Epoch [4/5], Step [2441/2603], Loss: 0.8848980665206909\n",
            "Epoch [4/5], Step [2451/2603], Loss: 0.9340884685516357\n",
            "Epoch [4/5], Step [2461/2603], Loss: 0.9132681488990784\n",
            "Epoch [4/5], Step [2471/2603], Loss: 0.8502179980278015\n",
            "Epoch [4/5], Step [2481/2603], Loss: 0.8412072062492371\n",
            "Epoch [4/5], Step [2491/2603], Loss: 1.0467392206192017\n",
            "Epoch [4/5], Step [2501/2603], Loss: 0.9427889585494995\n",
            "Epoch [4/5], Step [2511/2603], Loss: 0.7728755474090576\n",
            "Epoch [4/5], Step [2521/2603], Loss: 0.9584099650382996\n",
            "Epoch [4/5], Step [2531/2603], Loss: 1.0193490982055664\n",
            "Epoch [4/5], Step [2541/2603], Loss: 0.9723120927810669\n",
            "Epoch [4/5], Step [2551/2603], Loss: 0.9138797521591187\n",
            "Epoch [4/5], Step [2561/2603], Loss: 1.1492764949798584\n",
            "Epoch [4/5], Step [2571/2603], Loss: 0.8637853860855103\n",
            "Epoch [4/5], Step [2581/2603], Loss: 1.28074312210083\n",
            "Epoch [4/5], Step [2591/2603], Loss: 0.9630898237228394\n",
            "Epoch [4/5], Step [2601/2603], Loss: 0.9633875489234924\n",
            "Epoch [5/5], Step [1/2603], Loss: 0.5504018068313599\n",
            "Epoch [5/5], Step [11/2603], Loss: 0.5467305779457092\n",
            "Epoch [5/5], Step [21/2603], Loss: 0.5648882985115051\n",
            "Epoch [5/5], Step [31/2603], Loss: 0.43428897857666016\n",
            "Epoch [5/5], Step [41/2603], Loss: 0.5074567198753357\n",
            "Epoch [5/5], Step [51/2603], Loss: 0.450981080532074\n",
            "Epoch [5/5], Step [61/2603], Loss: 0.524293839931488\n",
            "Epoch [5/5], Step [71/2603], Loss: 0.5190281867980957\n",
            "Epoch [5/5], Step [81/2603], Loss: 0.556213915348053\n",
            "Epoch [5/5], Step [91/2603], Loss: 0.581343948841095\n",
            "Epoch [5/5], Step [101/2603], Loss: 0.6393893957138062\n",
            "Epoch [5/5], Step [111/2603], Loss: 0.4237864911556244\n",
            "Epoch [5/5], Step [121/2603], Loss: 0.6721789836883545\n",
            "Epoch [5/5], Step [131/2603], Loss: 0.5540944933891296\n",
            "Epoch [5/5], Step [141/2603], Loss: 0.6290631294250488\n",
            "Epoch [5/5], Step [151/2603], Loss: 0.5146341323852539\n",
            "Epoch [5/5], Step [161/2603], Loss: 0.4218458831310272\n",
            "Epoch [5/5], Step [171/2603], Loss: 0.44409218430519104\n",
            "Epoch [5/5], Step [181/2603], Loss: 0.41021621227264404\n",
            "Epoch [5/5], Step [191/2603], Loss: 0.49614304304122925\n",
            "Epoch [5/5], Step [201/2603], Loss: 0.5853182673454285\n",
            "Epoch [5/5], Step [211/2603], Loss: 0.5767356157302856\n",
            "Epoch [5/5], Step [221/2603], Loss: 0.4651910364627838\n",
            "Epoch [5/5], Step [231/2603], Loss: 0.5581116080284119\n",
            "Epoch [5/5], Step [241/2603], Loss: 0.45877256989479065\n",
            "Epoch [5/5], Step [251/2603], Loss: 0.5339006781578064\n",
            "Epoch [5/5], Step [261/2603], Loss: 0.556191623210907\n",
            "Epoch [5/5], Step [271/2603], Loss: 0.5060160756111145\n",
            "Epoch [5/5], Step [281/2603], Loss: 0.4641396701335907\n",
            "Epoch [5/5], Step [291/2603], Loss: 0.5904515385627747\n",
            "Epoch [5/5], Step [301/2603], Loss: 0.5216621160507202\n",
            "Epoch [5/5], Step [311/2603], Loss: 0.5561965703964233\n",
            "Epoch [5/5], Step [321/2603], Loss: 0.6044108271598816\n",
            "Epoch [5/5], Step [331/2603], Loss: 0.49952998757362366\n",
            "Epoch [5/5], Step [341/2603], Loss: 0.6670702695846558\n",
            "Epoch [5/5], Step [351/2603], Loss: 0.5503203868865967\n",
            "Epoch [5/5], Step [361/2603], Loss: 0.533470630645752\n",
            "Epoch [5/5], Step [371/2603], Loss: 0.6807645559310913\n",
            "Epoch [5/5], Step [381/2603], Loss: 0.594481348991394\n",
            "Epoch [5/5], Step [391/2603], Loss: 0.5850033164024353\n",
            "Epoch [5/5], Step [401/2603], Loss: 0.4889437258243561\n",
            "Epoch [5/5], Step [411/2603], Loss: 0.4910726845264435\n",
            "Epoch [5/5], Step [421/2603], Loss: 0.45033249258995056\n",
            "Epoch [5/5], Step [431/2603], Loss: 0.5841774940490723\n",
            "Epoch [5/5], Step [441/2603], Loss: 0.5228231549263\n",
            "Epoch [5/5], Step [451/2603], Loss: 0.5682183504104614\n",
            "Epoch [5/5], Step [461/2603], Loss: 0.5559852719306946\n",
            "Epoch [5/5], Step [471/2603], Loss: 0.6105145812034607\n",
            "Epoch [5/5], Step [481/2603], Loss: 0.5637701749801636\n",
            "Epoch [5/5], Step [491/2603], Loss: 0.43918153643608093\n",
            "Epoch [5/5], Step [501/2603], Loss: 0.6183493137359619\n",
            "Epoch [5/5], Step [511/2603], Loss: 0.4856170117855072\n",
            "Epoch [5/5], Step [521/2603], Loss: 0.5862594246864319\n",
            "Epoch [5/5], Step [531/2603], Loss: 0.6626215577125549\n",
            "Epoch [5/5], Step [541/2603], Loss: 0.5237118005752563\n",
            "Epoch [5/5], Step [551/2603], Loss: 0.5614802241325378\n",
            "Epoch [5/5], Step [561/2603], Loss: 0.5003105401992798\n",
            "Epoch [5/5], Step [571/2603], Loss: 0.5500784516334534\n",
            "Epoch [5/5], Step [581/2603], Loss: 0.6448391675949097\n",
            "Epoch [5/5], Step [591/2603], Loss: 0.5406875610351562\n",
            "Epoch [5/5], Step [601/2603], Loss: 0.48213428258895874\n",
            "Epoch [5/5], Step [611/2603], Loss: 0.5658696889877319\n",
            "Epoch [5/5], Step [621/2603], Loss: 0.6072630882263184\n",
            "Epoch [5/5], Step [631/2603], Loss: 0.5847888588905334\n",
            "Epoch [5/5], Step [641/2603], Loss: 0.6087866425514221\n",
            "Epoch [5/5], Step [651/2603], Loss: 0.5018734335899353\n",
            "Epoch [5/5], Step [661/2603], Loss: 0.5481258630752563\n",
            "Epoch [5/5], Step [671/2603], Loss: 0.6946030855178833\n",
            "Epoch [5/5], Step [681/2603], Loss: 0.6147887110710144\n",
            "Epoch [5/5], Step [691/2603], Loss: 0.6465839743614197\n",
            "Epoch [5/5], Step [701/2603], Loss: 0.6592326760292053\n",
            "Epoch [5/5], Step [711/2603], Loss: 0.6607689261436462\n",
            "Epoch [5/5], Step [721/2603], Loss: 0.4313358664512634\n",
            "Epoch [5/5], Step [731/2603], Loss: 0.589168131351471\n",
            "Epoch [5/5], Step [741/2603], Loss: 0.5368886590003967\n",
            "Epoch [5/5], Step [751/2603], Loss: 0.5902011394500732\n",
            "Epoch [5/5], Step [761/2603], Loss: 0.5791813731193542\n",
            "Epoch [5/5], Step [771/2603], Loss: 0.6729966998100281\n",
            "Epoch [5/5], Step [781/2603], Loss: 0.5445398688316345\n",
            "Epoch [5/5], Step [791/2603], Loss: 0.6536046266555786\n",
            "Epoch [5/5], Step [801/2603], Loss: 0.8028165102005005\n",
            "Epoch [5/5], Step [811/2603], Loss: 0.6569582223892212\n",
            "Epoch [5/5], Step [821/2603], Loss: 0.7130703926086426\n",
            "Epoch [5/5], Step [831/2603], Loss: 0.7544689774513245\n",
            "Epoch [5/5], Step [841/2603], Loss: 0.6713921427726746\n",
            "Epoch [5/5], Step [851/2603], Loss: 0.6475375294685364\n",
            "Epoch [5/5], Step [861/2603], Loss: 0.6993510723114014\n",
            "Epoch [5/5], Step [871/2603], Loss: 0.5938519239425659\n",
            "Epoch [5/5], Step [881/2603], Loss: 0.5431150794029236\n",
            "Epoch [5/5], Step [891/2603], Loss: 0.5069260597229004\n",
            "Epoch [5/5], Step [901/2603], Loss: 0.5466324090957642\n",
            "Epoch [5/5], Step [911/2603], Loss: 0.6549695730209351\n",
            "Epoch [5/5], Step [921/2603], Loss: 0.7050309181213379\n",
            "Epoch [5/5], Step [931/2603], Loss: 0.6275280714035034\n",
            "Epoch [5/5], Step [941/2603], Loss: 0.5551712512969971\n",
            "Epoch [5/5], Step [951/2603], Loss: 0.3651261329650879\n",
            "Epoch [5/5], Step [961/2603], Loss: 0.5285587906837463\n",
            "Epoch [5/5], Step [971/2603], Loss: 0.5996226072311401\n",
            "Epoch [5/5], Step [981/2603], Loss: 0.5757334232330322\n",
            "Epoch [5/5], Step [991/2603], Loss: 0.7047367095947266\n",
            "Epoch [5/5], Step [1001/2603], Loss: 0.6384549140930176\n",
            "Epoch [5/5], Step [1011/2603], Loss: 0.4909602105617523\n",
            "Epoch [5/5], Step [1021/2603], Loss: 0.5721256136894226\n",
            "Epoch [5/5], Step [1031/2603], Loss: 0.63924241065979\n",
            "Epoch [5/5], Step [1041/2603], Loss: 0.6913039684295654\n",
            "Epoch [5/5], Step [1051/2603], Loss: 0.6778691411018372\n",
            "Epoch [5/5], Step [1061/2603], Loss: 0.855562686920166\n",
            "Epoch [5/5], Step [1071/2603], Loss: 0.8006988167762756\n",
            "Epoch [5/5], Step [1081/2603], Loss: 0.5788567662239075\n",
            "Epoch [5/5], Step [1091/2603], Loss: 0.5696566104888916\n",
            "Epoch [5/5], Step [1101/2603], Loss: 0.6643739342689514\n",
            "Epoch [5/5], Step [1111/2603], Loss: 0.6819164752960205\n",
            "Epoch [5/5], Step [1121/2603], Loss: 0.6088932156562805\n",
            "Epoch [5/5], Step [1131/2603], Loss: 0.4592284858226776\n",
            "Epoch [5/5], Step [1141/2603], Loss: 0.6136343479156494\n",
            "Epoch [5/5], Step [1151/2603], Loss: 0.6797786355018616\n",
            "Epoch [5/5], Step [1161/2603], Loss: 0.720491349697113\n",
            "Epoch [5/5], Step [1171/2603], Loss: 0.6705235242843628\n",
            "Epoch [5/5], Step [1181/2603], Loss: 0.7179415822029114\n",
            "Epoch [5/5], Step [1191/2603], Loss: 0.6431787014007568\n",
            "Epoch [5/5], Step [1201/2603], Loss: 0.5496945381164551\n",
            "Epoch [5/5], Step [1211/2603], Loss: 0.6238177418708801\n",
            "Epoch [5/5], Step [1221/2603], Loss: 0.6539210081100464\n",
            "Epoch [5/5], Step [1231/2603], Loss: 0.6595781445503235\n",
            "Epoch [5/5], Step [1241/2603], Loss: 0.530799925327301\n",
            "Epoch [5/5], Step [1251/2603], Loss: 0.5601475238800049\n",
            "Epoch [5/5], Step [1261/2603], Loss: 0.5966445803642273\n",
            "Epoch [5/5], Step [1271/2603], Loss: 0.6588146686553955\n",
            "Epoch [5/5], Step [1281/2603], Loss: 0.6446515917778015\n",
            "Epoch [5/5], Step [1291/2603], Loss: 0.592137336730957\n",
            "Epoch [5/5], Step [1301/2603], Loss: 0.5528481006622314\n",
            "Epoch [5/5], Step [1311/2603], Loss: 0.49863511323928833\n",
            "Epoch [5/5], Step [1321/2603], Loss: 0.5959132313728333\n",
            "Epoch [5/5], Step [1331/2603], Loss: 0.5586708188056946\n",
            "Epoch [5/5], Step [1341/2603], Loss: 0.6758775115013123\n",
            "Epoch [5/5], Step [1351/2603], Loss: 0.6971150040626526\n",
            "Epoch [5/5], Step [1361/2603], Loss: 0.6688409447669983\n",
            "Epoch [5/5], Step [1371/2603], Loss: 0.6433064937591553\n",
            "Epoch [5/5], Step [1381/2603], Loss: 0.7402037382125854\n",
            "Epoch [5/5], Step [1391/2603], Loss: 0.6208062767982483\n",
            "Epoch [5/5], Step [1401/2603], Loss: 0.6095529198646545\n",
            "Epoch [5/5], Step [1411/2603], Loss: 0.5096486806869507\n",
            "Epoch [5/5], Step [1421/2603], Loss: 0.7967263460159302\n",
            "Epoch [5/5], Step [1431/2603], Loss: 0.7408345937728882\n",
            "Epoch [5/5], Step [1441/2603], Loss: 0.5700263977050781\n",
            "Epoch [5/5], Step [1451/2603], Loss: 0.7742675542831421\n",
            "Epoch [5/5], Step [1461/2603], Loss: 0.6525251865386963\n",
            "Epoch [5/5], Step [1471/2603], Loss: 0.5943805575370789\n",
            "Epoch [5/5], Step [1481/2603], Loss: 0.7651732563972473\n",
            "Epoch [5/5], Step [1491/2603], Loss: 0.5867575407028198\n",
            "Epoch [5/5], Step [1501/2603], Loss: 0.6984747052192688\n",
            "Epoch [5/5], Step [1511/2603], Loss: 0.7260473370552063\n",
            "Epoch [5/5], Step [1521/2603], Loss: 0.7845220565795898\n",
            "Epoch [5/5], Step [1531/2603], Loss: 0.6498026251792908\n",
            "Epoch [5/5], Step [1541/2603], Loss: 0.6024758815765381\n",
            "Epoch [5/5], Step [1551/2603], Loss: 0.568215548992157\n",
            "Epoch [5/5], Step [1561/2603], Loss: 0.5512582063674927\n",
            "Epoch [5/5], Step [1571/2603], Loss: 0.8318113088607788\n",
            "Epoch [5/5], Step [1581/2603], Loss: 0.549133837223053\n",
            "Epoch [5/5], Step [1591/2603], Loss: 0.6878296136856079\n",
            "Epoch [5/5], Step [1601/2603], Loss: 0.5845446586608887\n",
            "Epoch [5/5], Step [1611/2603], Loss: 0.6749702095985413\n",
            "Epoch [5/5], Step [1621/2603], Loss: 0.5128819346427917\n",
            "Epoch [5/5], Step [1631/2603], Loss: 0.6045417785644531\n",
            "Epoch [5/5], Step [1641/2603], Loss: 0.7709023952484131\n",
            "Epoch [5/5], Step [1651/2603], Loss: 0.634841799736023\n",
            "Epoch [5/5], Step [1661/2603], Loss: 0.6586631536483765\n",
            "Epoch [5/5], Step [1671/2603], Loss: 0.6454343795776367\n",
            "Epoch [5/5], Step [1681/2603], Loss: 0.6134008169174194\n",
            "Epoch [5/5], Step [1691/2603], Loss: 0.5785934329032898\n",
            "Epoch [5/5], Step [1701/2603], Loss: 0.7146788239479065\n",
            "Epoch [5/5], Step [1711/2603], Loss: 0.6460241675376892\n",
            "Epoch [5/5], Step [1721/2603], Loss: 0.5675710439682007\n",
            "Epoch [5/5], Step [1731/2603], Loss: 0.5543816089630127\n",
            "Epoch [5/5], Step [1741/2603], Loss: 0.5420300960540771\n",
            "Epoch [5/5], Step [1751/2603], Loss: 0.7643647193908691\n",
            "Epoch [5/5], Step [1761/2603], Loss: 0.5878152251243591\n",
            "Epoch [5/5], Step [1771/2603], Loss: 0.7345359921455383\n",
            "Epoch [5/5], Step [1781/2603], Loss: 0.6885380744934082\n",
            "Epoch [5/5], Step [1791/2603], Loss: 0.6661028861999512\n",
            "Epoch [5/5], Step [1801/2603], Loss: 0.4480481445789337\n",
            "Epoch [5/5], Step [1811/2603], Loss: 0.5455396771430969\n",
            "Epoch [5/5], Step [1821/2603], Loss: 0.8076719641685486\n",
            "Epoch [5/5], Step [1831/2603], Loss: 0.7934088706970215\n",
            "Epoch [5/5], Step [1841/2603], Loss: 0.8362613916397095\n",
            "Epoch [5/5], Step [1851/2603], Loss: 0.818263828754425\n",
            "Epoch [5/5], Step [1861/2603], Loss: 0.5662981867790222\n",
            "Epoch [5/5], Step [1871/2603], Loss: 0.7784221172332764\n",
            "Epoch [5/5], Step [1881/2603], Loss: 0.6024735569953918\n",
            "Epoch [5/5], Step [1891/2603], Loss: 0.7554863691329956\n",
            "Epoch [5/5], Step [1901/2603], Loss: 0.4672383964061737\n",
            "Epoch [5/5], Step [1911/2603], Loss: 0.7165006399154663\n",
            "Epoch [5/5], Step [1921/2603], Loss: 0.6404797434806824\n",
            "Epoch [5/5], Step [1931/2603], Loss: 0.6274656653404236\n",
            "Epoch [5/5], Step [1941/2603], Loss: 0.5843105912208557\n",
            "Epoch [5/5], Step [1951/2603], Loss: 0.6453360319137573\n",
            "Epoch [5/5], Step [1961/2603], Loss: 0.5948958992958069\n",
            "Epoch [5/5], Step [1971/2603], Loss: 0.4635414183139801\n",
            "Epoch [5/5], Step [1981/2603], Loss: 0.7613183259963989\n",
            "Epoch [5/5], Step [1991/2603], Loss: 0.6018133759498596\n",
            "Epoch [5/5], Step [2001/2603], Loss: 0.6654139161109924\n",
            "Epoch [5/5], Step [2011/2603], Loss: 0.6119786500930786\n",
            "Epoch [5/5], Step [2021/2603], Loss: 0.6825730204582214\n",
            "Epoch [5/5], Step [2031/2603], Loss: 0.6436802744865417\n",
            "Epoch [5/5], Step [2041/2603], Loss: 0.5211682319641113\n",
            "Epoch [5/5], Step [2051/2603], Loss: 0.6298020482063293\n",
            "Epoch [5/5], Step [2061/2603], Loss: 0.7322694063186646\n",
            "Epoch [5/5], Step [2071/2603], Loss: 0.7128171324729919\n",
            "Epoch [5/5], Step [2081/2603], Loss: 0.5887134075164795\n",
            "Epoch [5/5], Step [2091/2603], Loss: 0.5240025520324707\n",
            "Epoch [5/5], Step [2101/2603], Loss: 0.5056308507919312\n",
            "Epoch [5/5], Step [2111/2603], Loss: 0.679203987121582\n",
            "Epoch [5/5], Step [2121/2603], Loss: 0.776611864566803\n",
            "Epoch [5/5], Step [2131/2603], Loss: 0.6649751663208008\n",
            "Epoch [5/5], Step [2141/2603], Loss: 0.6700266599655151\n",
            "Epoch [5/5], Step [2151/2603], Loss: 0.7102120518684387\n",
            "Epoch [5/5], Step [2161/2603], Loss: 0.7215627431869507\n",
            "Epoch [5/5], Step [2171/2603], Loss: 0.6970655918121338\n",
            "Epoch [5/5], Step [2181/2603], Loss: 0.7096222639083862\n",
            "Epoch [5/5], Step [2191/2603], Loss: 0.7860038876533508\n",
            "Epoch [5/5], Step [2201/2603], Loss: 0.6136826872825623\n",
            "Epoch [5/5], Step [2211/2603], Loss: 0.5490094423294067\n",
            "Epoch [5/5], Step [2221/2603], Loss: 0.6592293977737427\n",
            "Epoch [5/5], Step [2231/2603], Loss: 0.6619823575019836\n",
            "Epoch [5/5], Step [2241/2603], Loss: 0.7186328768730164\n",
            "Epoch [5/5], Step [2251/2603], Loss: 0.7079143524169922\n",
            "Epoch [5/5], Step [2261/2603], Loss: 0.5995336174964905\n",
            "Epoch [5/5], Step [2271/2603], Loss: 0.7775790095329285\n",
            "Epoch [5/5], Step [2281/2603], Loss: 0.6733879446983337\n",
            "Epoch [5/5], Step [2291/2603], Loss: 0.6660906076431274\n",
            "Epoch [5/5], Step [2301/2603], Loss: 0.7087912559509277\n",
            "Epoch [5/5], Step [2311/2603], Loss: 0.7529966831207275\n",
            "Epoch [5/5], Step [2321/2603], Loss: 0.683264970779419\n",
            "Epoch [5/5], Step [2331/2603], Loss: 0.8434436917304993\n",
            "Epoch [5/5], Step [2341/2603], Loss: 0.7322009205818176\n",
            "Epoch [5/5], Step [2351/2603], Loss: 0.6259945631027222\n",
            "Epoch [5/5], Step [2361/2603], Loss: 0.7138832211494446\n",
            "Epoch [5/5], Step [2371/2603], Loss: 0.6441717743873596\n",
            "Epoch [5/5], Step [2381/2603], Loss: 0.7078169584274292\n",
            "Epoch [5/5], Step [2391/2603], Loss: 0.6250624060630798\n",
            "Epoch [5/5], Step [2401/2603], Loss: 0.6682741641998291\n",
            "Epoch [5/5], Step [2411/2603], Loss: 0.7482072710990906\n",
            "Epoch [5/5], Step [2421/2603], Loss: 0.6249842643737793\n",
            "Epoch [5/5], Step [2431/2603], Loss: 0.7155452966690063\n",
            "Epoch [5/5], Step [2441/2603], Loss: 0.6038373112678528\n",
            "Epoch [5/5], Step [2451/2603], Loss: 0.5937420725822449\n",
            "Epoch [5/5], Step [2461/2603], Loss: 0.7129402160644531\n",
            "Epoch [5/5], Step [2471/2603], Loss: 0.7175294756889343\n",
            "Epoch [5/5], Step [2481/2603], Loss: 0.7334479093551636\n",
            "Epoch [5/5], Step [2491/2603], Loss: 0.6345075964927673\n",
            "Epoch [5/5], Step [2501/2603], Loss: 0.6564886569976807\n",
            "Epoch [5/5], Step [2511/2603], Loss: 0.8141596913337708\n",
            "Epoch [5/5], Step [2521/2603], Loss: 0.5871521234512329\n",
            "Epoch [5/5], Step [2531/2603], Loss: 0.6676641702651978\n",
            "Epoch [5/5], Step [2541/2603], Loss: 0.8078342080116272\n",
            "Epoch [5/5], Step [2551/2603], Loss: 0.6546213626861572\n",
            "Epoch [5/5], Step [2561/2603], Loss: 0.8030038475990295\n",
            "Epoch [5/5], Step [2571/2603], Loss: 0.7843129634857178\n",
            "Epoch [5/5], Step [2581/2603], Loss: 0.6743574738502502\n",
            "Epoch [5/5], Step [2591/2603], Loss: 0.6878700852394104\n",
            "Epoch [5/5], Step [2601/2603], Loss: 0.8148133754730225\n",
            "Test Loss: 1.7711054396044377\n",
            "Original: Baarl olman istiyorum .\n",
            "Translation: , i want you to succeed . . one . who are successful . hunting\n",
            "\n",
            "Original: Belediye bakanlna tekrardan seildi .\n",
            "Translation: was elected with taxes . ? ! . entire in the company . marrying .\n",
            "\n",
            "Original: Tom her yl ziyaret etmeye gelir .\n",
            "Translation: will come to visit every year . ? guys . n't . up . by\n",
            "\n",
            "Original: Almanya 'da hayat nasl ?\n",
            "Translation: in germany ? ! how 's life . could you feel . `` . ``\n",
            "\n",
            "Original: Bit yardmc olman gerekiyor .\n",
            "Translation: should be helps . ? good . blog . opposition . gravel . hunting .\n",
            "\n",
            "Original: Tom bana bizimle gitmek istemediini syledi .\n",
            "Translation: to me that tom did n't want to go with us . booster . booster\n",
            "\n",
            "Original: Onu daha nce duydum .\n",
            "Translation: it before . i 've heard that . seat . staring . staring . staring\n",
            "\n",
            "Original: Kurtulduk .\n",
            "Translation: were enough . you were not . it . in the care . `` .\n",
            "\n",
            "Original: Tom neredeyse hi TV izlemez .\n",
            "Translation: hardly ever , tom is almost watching tv . precise . portable . portable .\n",
            "\n",
            "Original: Bu tr filmle ilgilenmiyorum .\n",
            "Translation: are not in this kind of drink . ? . some of thing . .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import nltk\n",
        "import random\n",
        "import tqdm\n",
        "import pickle\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Encoder model\n",
        "class Encoder_Model(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, devices, batch_size):\n",
        "        super(Encoder_Model, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.devices = devices\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = int(self.d_model / num_heads)\n",
        "\n",
        "        # Multihead Attention\n",
        "        self.query = nn.Linear(self.d_model, self.d_model)\n",
        "        self.key = nn.Linear(self.d_model, self.d_model)\n",
        "        self.value = nn.Linear(self.d_model, self.d_model)\n",
        "        self.concat_scaled_dot_product = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # Feed Forward\n",
        "        self.feed_forward1 = nn.Linear(self.d_model, 2048)\n",
        "        self.feed_forward2 = nn.Linear(2048, self.d_model)\n",
        "\n",
        "    # Multihead Attention\n",
        "    def Multihead_Attention(self, data):\n",
        "        batch_size, seq_len, _ = data.shape\n",
        "        query = self.query(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = self.key(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = self.value(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        dot_product = torch.matmul(query, torch.transpose(key, -2, -1)) / ((self.d_k) ** 0.5)\n",
        "        scaled_dot = torch.nn.functional.softmax(dot_product, dim=-1)\n",
        "\n",
        "        scaled_dot = torch.matmul(scaled_dot, value)\n",
        "        scaled_dot = scaled_dot.permute(0, 2, 1, 3)\n",
        "\n",
        "        concat_scaled_dot_product = scaled_dot.reshape(batch_size, seq_len, -1)\n",
        "        concat_scaled_dot_product = self.concat_scaled_dot_product(concat_scaled_dot_product)\n",
        "\n",
        "        return concat_scaled_dot_product\n",
        "\n",
        "    # Feed Forward\n",
        "    def Feed_Forward(self, data):\n",
        "        data = self.feed_forward1(data)\n",
        "        data = torch.nn.functional.relu(data)\n",
        "        data = self.feed_forward2(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def forward(self, data):\n",
        "        mhe_data = self.Multihead_Attention(data=data)\n",
        "        norm_data = nn.functional.layer_norm((mhe_data + data), normalized_shape=mhe_data.shape)\n",
        "\n",
        "        feed_forward = self.Feed_Forward(data=norm_data)\n",
        "        data = nn.functional.layer_norm((norm_data + feed_forward), normalized_shape=feed_forward.shape)\n",
        "\n",
        "        return data\n",
        "\n",
        "# Decoder model\n",
        "class Decoder_Model(nn.Module):\n",
        "    def __init__(self, devices, d_model, num_heads, batch_size, masking_value=-1e8):\n",
        "        super(Decoder_Model, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.masking_value = masking_value\n",
        "        self.devices = devices\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = int(self.d_model / num_heads)\n",
        "\n",
        "        # Masked Multi Head Attention\n",
        "        self.query_m = nn.Linear(self.d_model, d_model)\n",
        "        self.key_m = nn.Linear(self.d_model, self.d_model)\n",
        "        self.value_m = nn.Linear(self.d_model, self.d_model)\n",
        "        self.concat_scaled_dot_product_m = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # Multihead Attention\n",
        "        self.query = nn.Linear(self.d_model, self.d_model)\n",
        "        self.key = nn.Linear(self.d_model, self.d_model)\n",
        "        self.value = nn.Linear(self.d_model, self.d_model)\n",
        "        self.concat_scaled_dot_product = nn.Linear(self.d_model, self.d_model)\n",
        "\n",
        "        # Feed Forward\n",
        "        self.feed_forward1 = nn.Linear(self.d_model, 2048)\n",
        "        self.feed_forward2 = nn.Linear(2048, self.d_model)\n",
        "\n",
        "    # Masked Multi Head Attention\n",
        "    def Masked_Multihead_Attention(self, data):\n",
        "        batch_size, seq_len, _ = data.shape\n",
        "        query = self.query_m(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = self.key_m(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = self.value_m(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        dot_product = torch.matmul(query, torch.transpose(key, -1, -2)) / (self.d_k ** 0.5)\n",
        "\n",
        "        mask = torch.triu(torch.ones_like(dot_product), diagonal=1).to(self.devices)\n",
        "        mask_data = self.masking_value * mask\n",
        "\n",
        "        masked_product = mask_data + dot_product\n",
        "        scaled_dot = torch.nn.functional.softmax(masked_product, dim=-1)\n",
        "        scaled_dot = torch.matmul(scaled_dot, value)\n",
        "\n",
        "        scaled_dot = scaled_dot.permute(0, 2, 1, 3)\n",
        "        concat_scaled_dot_product = scaled_dot.reshape(batch_size, seq_len, -1)\n",
        "        concat_scaled_dot_product = self.concat_scaled_dot_product_m(concat_scaled_dot_product)\n",
        "\n",
        "        return concat_scaled_dot_product\n",
        "\n",
        "    # Multi Head Attention\n",
        "    def Multihead_Attention(self, data, encoder_out):\n",
        "        batch_size, seq_len, _ = data.shape\n",
        "        _, encoder_seq_len, _ = encoder_out.shape\n",
        "        query = self.query(data).reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = self.key(encoder_out).reshape(batch_size, encoder_seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = self.value(encoder_out).reshape(batch_size, encoder_seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        dot_product = torch.matmul(query, torch.transpose(key, -1, -2)) / ((self.d_k) ** 0.5)\n",
        "        scaled_dot = torch.nn.functional.softmax(dot_product, dim=-1)\n",
        "        scaled_dot = torch.matmul(scaled_dot, value)\n",
        "\n",
        "        scaled_dot = scaled_dot.permute(0, 2, 1, 3)\n",
        "        concat_scaled_dot_product = scaled_dot.reshape(batch_size, seq_len, -1)\n",
        "        concat_scaled_dot_product = self.concat_scaled_dot_product(concat_scaled_dot_product)\n",
        "\n",
        "        return concat_scaled_dot_product\n",
        "\n",
        "    # Feed Forward\n",
        "    def Feed_Forward(self, data):\n",
        "        data = self.feed_forward1(data)\n",
        "        data = torch.nn.functional.relu(data)\n",
        "        data = self.feed_forward2(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def forward(self, data):\n",
        "        encoder_out, data_dec = data\n",
        "\n",
        "        mmhe_data = self.Masked_Multihead_Attention(data=data_dec)\n",
        "        norm_mmhe = nn.functional.layer_norm((mmhe_data + data_dec), normalized_shape=mmhe_data.shape)\n",
        "\n",
        "        mhe_data = self.Multihead_Attention(data=norm_mmhe, encoder_out=encoder_out)\n",
        "        norm_mhe = nn.functional.layer_norm((mhe_data + norm_mmhe), normalized_shape=mhe_data.shape)\n",
        "\n",
        "        feed_forward = self.Feed_Forward(data=norm_mhe)\n",
        "        data_dec = nn.functional.layer_norm((norm_mhe + feed_forward), normalized_shape=feed_forward.shape)\n",
        "\n",
        "        return data_dec\n",
        "\n",
        "# Embedding model\n",
        "class Embedding_Model(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, pad_idx, devices, max_seq_len):\n",
        "        super(Embedding_Model, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.devices = devices\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=pad_idx)\n",
        "\n",
        "    def Positional_Encoding(self, seq_len):\n",
        "        position = torch.arange(0, seq_len).reshape((seq_len, 1)).to(self.devices)\n",
        "        even_i = torch.arange(0, self.d_model, 2).to(self.devices)\n",
        "        odd_i = torch.arange(1, self.d_model, 2).to(self.devices)\n",
        "\n",
        "        pow_even = torch.pow(10000, -even_i / self.d_model)\n",
        "        pow_odd = torch.pow(10000, -odd_i / self.d_model)\n",
        "\n",
        "        PE_even = torch.sin(position * pow_even)\n",
        "        PE_odd = torch.cos(position * pow_odd)\n",
        "\n",
        "        PE = torch.zeros((seq_len, self.d_model), device=self.devices)\n",
        "        PE[:, even_i] = PE_even\n",
        "        PE[:, odd_i] = PE_odd\n",
        "\n",
        "        return PE.unsqueeze(0)  # Adding batch dimension\n",
        "\n",
        "    def forward(self, data):\n",
        "        embedded_data = self.embedding(data)\n",
        "        seq_len = embedded_data.size(1)\n",
        "        PE = self.Positional_Encoding(seq_len)\n",
        "\n",
        "        return embedded_data + PE\n",
        "\n",
        "# Dataset\n",
        "class Translate_Dataset(Dataset):\n",
        "    def __init__(self, dataset, word2index_source, word2index_target, max_seq_len, devices):\n",
        "        self.dataset = dataset\n",
        "        self.word2index_source = word2index_source\n",
        "        self.word2index_target = word2index_target\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.devices = devices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def tokenize_and_pad(self, sentence, word2index):\n",
        "        tokens = nltk.word_tokenize(sentence.lower())\n",
        "        indices = [word2index.get(word, word2index['<unk>']) for word in tokens]\n",
        "        if len(indices) < self.max_seq_len:\n",
        "            indices += [word2index['<pad>']] * (self.max_seq_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_seq_len]\n",
        "        return torch.tensor(indices, device=self.devices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_sentence, target_sentence = self.dataset[idx]\n",
        "        source_indices = self.tokenize_and_pad(source_sentence, self.word2index_source)\n",
        "        target_indices = self.tokenize_and_pad(target_sentence, self.word2index_target)\n",
        "\n",
        "        return source_indices, target_indices\n",
        "\n",
        "# Transformer Model\n",
        "class Transformer_Model(nn.Module):\n",
        "    def __init__(self, devices, vocab_size_source, vocab_size_target, max_seq_len, pad_idx, d_model, num_heads, num_encoder_layers, num_decoder_layers, batch_size):\n",
        "        super(Transformer_Model, self).__init__()\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([Encoder_Model(d_model=d_model, num_heads=num_heads, devices=devices, batch_size=batch_size) for _ in range(num_encoder_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([Decoder_Model(devices=devices, d_model=d_model, num_heads=num_heads, batch_size=batch_size) for _ in range(num_decoder_layers)])\n",
        "\n",
        "        self.source_embedding = Embedding_Model(vocab_size=vocab_size_source, d_model=d_model, pad_idx=pad_idx, devices=devices, max_seq_len=max_seq_len)\n",
        "        self.target_embedding = Embedding_Model(vocab_size=vocab_size_target, d_model=d_model, pad_idx=pad_idx, devices=devices, max_seq_len=max_seq_len)\n",
        "\n",
        "        self.linear = nn.Linear(d_model, vocab_size_target)\n",
        "\n",
        "    def forward(self, data):\n",
        "        source_data, target_data = data\n",
        "\n",
        "        encoder_data = self.source_embedding(source_data)\n",
        "        decoder_data = self.target_embedding(target_data)\n",
        "\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_data = encoder_layer(data=encoder_data)\n",
        "\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_data = decoder_layer(data=(encoder_data, decoder_data))\n",
        "\n",
        "        output = self.linear(decoder_data)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Data Loading Functions\n",
        "def load_data(turkish_file, english_file):\n",
        "    with open(turkish_file, 'r', encoding='utf-8') as f:\n",
        "        turkish_sentences = f.readlines()\n",
        "\n",
        "    with open(english_file, 'r', encoding='utf-8') as f:\n",
        "        english_sentences = f.readlines()\n",
        "\n",
        "    data = list(zip(turkish_sentences, english_sentences))\n",
        "    return data\n",
        "\n",
        "def build_vocab(sentences, min_freq=1):\n",
        "    word_freq = {}\n",
        "    for sentence in sentences:\n",
        "        for word in nltk.word_tokenize(sentence.lower()):\n",
        "            if word not in word_freq:\n",
        "                word_freq[word] = 1\n",
        "            else:\n",
        "                word_freq[word] += 1\n",
        "\n",
        "    word2index = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            word2index[word] = len(word2index)\n",
        "\n",
        "    return word2index\n",
        "\n",
        "# Load Data\n",
        "data = load_data('train.tr.txt', 'train.en.txt')\n",
        "\n",
        "# Build Vocabulary\n",
        "turkish_sentences = [pair[0] for pair in data]\n",
        "english_sentences = [pair[1] for pair in data]\n",
        "\n",
        "word2index_source = build_vocab(turkish_sentences)\n",
        "word2index_target = build_vocab(english_sentences)\n",
        "\n",
        "# Save vocabulary for future use\n",
        "with open('word2index_source.pkl', 'wb') as f:\n",
        "    pickle.dump(word2index_source, f)\n",
        "\n",
        "with open('word2index_target.pkl', 'wb') as f:\n",
        "    pickle.dump(word2index_target, f)\n",
        "\n",
        "# Split Data into Train and Test Sets\n",
        "random.shuffle(data)\n",
        "split_idx = int(len(data) * 0.3)\n",
        "train_data = data[:split_idx]\n",
        "test_data = data[split_idx:]\n",
        "\n",
        "# Set Parameters\n",
        "max_seq_len = 16\n",
        "devices = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 64\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = Translate_Dataset(train_data, word2index_source, word2index_target, max_seq_len, devices)\n",
        "test_dataset = Translate_Dataset(test_data, word2index_source, word2index_target, max_seq_len, devices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize Model\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "pad_idx = 0\n",
        "vocab_size_source = len(word2index_source)\n",
        "vocab_size_target = len(word2index_target)\n",
        "\n",
        "model = Transformer_Model(devices=devices, vocab_size_source=vocab_size_source, vocab_size_target=vocab_size_target, max_seq_len=max_seq_len, pad_idx=pad_idx, d_model=d_model, num_heads=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, batch_size=batch_size)\n",
        "model = model.to(devices)\n",
        "\n",
        "# Training Setup\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (source, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        source = source.to(devices).long()\n",
        "        target = target.to(devices).long()\n",
        "\n",
        "        output = model((source, target[:, :-1]))\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = target[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'transformer_model.pth')\n",
        "\n",
        "# Evaluation on the Test Set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    for i, (source, target) in enumerate(test_loader):\n",
        "        source = source.to(devices).long()\n",
        "        target = target.to(devices).long()\n",
        "\n",
        "        output = model((source, target[:, :-1]))\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target = target[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {total_loss / len(test_loader)}\")\n",
        "\n",
        "# Translation Function\n",
        "def translate_sentence(model, sentence, word2index_source, index2word_target, max_seq_len, devices):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize and convert to indices\n",
        "        tokens = nltk.word_tokenize(sentence.lower())\n",
        "        indices = [word2index_source.get(word, word2index_source['<unk>']) for word in tokens]\n",
        "        if len(indices) < max_seq_len:\n",
        "            indices += [word2index_source['<pad>']] * (max_seq_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:max_seq_len]\n",
        "        source_indices = torch.tensor(indices, device=devices).unsqueeze(0).long()\n",
        "\n",
        "        # Prepare target tensor with <sos> token\n",
        "        target_indices = torch.tensor([word2index_target['<pad>']] * max_seq_len, device=devices).unsqueeze(0).long()\n",
        "        target_indices[0, 0] = word2index_target['<sos>']\n",
        "\n",
        "        # Translate word by word\n",
        "        for i in range(1, max_seq_len):\n",
        "            output = model((source_indices, target_indices[:, :i]))\n",
        "            next_word_idx = torch.argmax(output[:, -1, :], dim=-1)\n",
        "            target_indices[0, i] = next_word_idx.item()\n",
        "            if next_word_idx.item() == word2index_target['<eos>']:\n",
        "                break\n",
        "\n",
        "        # Convert indices back to words\n",
        "        translated_sentence = [index2word_target[idx.item()] for idx in target_indices[0] if idx.item() not in {word2index_target['<pad>'], word2index_target['<sos>'], word2index_target['<eos>']}]\n",
        "        return ' '.join(translated_sentence)\n",
        "\n",
        "# Translate 10 random sentences\n",
        "random_sentences = random.sample(turkish_sentences, 10)\n",
        "\n",
        "# Create index2word mapping for the target vocabulary\n",
        "index2word_target = {index: word for word, index in word2index_target.items()}\n",
        "\n",
        "# Translate and print each sentence\n",
        "translations = []\n",
        "for sentence in random_sentences:\n",
        "    translation = translate_sentence(model, sentence, word2index_source, index2word_target, max_seq_len, devices)\n",
        "    translations.append((sentence.strip(), translation))\n",
        "    print(f\"Original: {sentence.strip()}\")\n",
        "    print(f\"Translation: {translation}\\n\")\n",
        "\n",
        "# Save translations\n",
        "with open('translations.txt', 'w', encoding='utf-8') as f:\n",
        "    for original, translation in translations:\n",
        "        f.write(f\"Original: {original}\\n\")\n",
        "        f.write(f\"Translation: {translation}\\n\\n\")\n"
      ]
    }
  ]
}